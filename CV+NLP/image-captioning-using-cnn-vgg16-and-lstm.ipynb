{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import re\n",
    "import numpy as np\n",
    "# gives UI to see how much the data is processed till now\n",
    "from tqdm import tqdm as tqdm\n",
    "\n",
    "from tensorflow.keras.applications.vgg16 import VGG16, preprocess_input\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.utils import to_categorical, plot_model\n",
    "from tensorflow.keras.layers import Input, Dense, LSTM, Embedding, Dropout, add\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR= '/kaggle/input/flicker-8k-image-dataset-captionstxt'\n",
    "WORKING_DIR= '/kaggle/working'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract image features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load VGG16 model\n",
    "\n",
    "model= VGG16()\n",
    "\n",
    "# restructure the model\n",
    "model= Model(inputs= model.inputs, outputs= model.layers[-2].output) # we dont need the FCN of the VGG16 model we just need \n",
    "                                                                    # previous layer for features\n",
    "    \n",
    "# summarize\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract features from image\n",
    "\n",
    "features= {}\n",
    "directory= os.path.join(BASE_DIR, 'Images')\n",
    "\n",
    "for img_name in tqdm(os.listdir(directory)):\n",
    "    # load the image from the file\n",
    "    img_path= directory+ '/' + img_name\n",
    "    image= load_img(img_path, target_size= (224,224))  # resizing the image \n",
    "    \n",
    "    # convert image pixels to numpy array\n",
    "    image= img_to_array(image)\n",
    "    # reshape the data for model\n",
    "    image= image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n",
    "    \n",
    "    # preprocess img for VGG \n",
    "    image= preprocess_input(image)\n",
    "    # extract features\n",
    "    feature= model.predict(image, verbose = 0)\n",
    "    \n",
    "    # get imag id\n",
    "    image_id= img_name.split('.')[0]\n",
    "    # store feature\n",
    "    features[image_id]= feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store features in pickle\n",
    "pickle.dump(features, open(os.path.join(WORKING_DIR, 'features.pkl'), 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load features from pickle\n",
    "with open(os.path.join(WORKING_DIR, 'features.pkl'), 'rb') as f:\n",
    "    features= pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features['3226254560_2f8ac147ea'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Caption Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(BASE_DIR, 'captions.txt'), 'r') as f:\n",
    "    # to skip the 1st line\n",
    "    next(f)\n",
    "    captions_doc= f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# captions_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create mapping of image to cations\n",
    "mapping= {}\n",
    "\n",
    "# process lines\n",
    "for line in tqdm(captions_doc.split('\\n')):\n",
    "    # split the line by comma\n",
    "    tokens= line.split(',')\n",
    "    \n",
    "    if len(line) < 2:\n",
    "        continue\n",
    "    image_id, caption= tokens[0], tokens[1:]\n",
    "    \n",
    "    # remove extension from imag_id\n",
    "    image_id= image_id.split('.')[0]\n",
    "    # convert cation list to string\n",
    "    caption=  ' '.join(caption)\n",
    "    # if theire are multiple cations for one image_id then store them all in one list of that image_id by creating list if needed\n",
    "    if image_id not in mapping:\n",
    "        mapping[image_id]= []\n",
    "    # store the caption\n",
    "    mapping[image_id].append(caption)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping[image_id][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess Text Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing the captions\n",
    "def clean_captions(mapping):\n",
    "    for key, captions in mapping.items():\n",
    "        for i in range(len(captions)):\n",
    "            # take one caption at a time\n",
    "            caption= captions[i]\n",
    "            \n",
    "            # preprocessing step\n",
    "            # conver to lower case\n",
    "            caption= caption.lower()\n",
    "            # have only alphabets\n",
    "            caption= caption.replace('[^A-Za-z]', '')\n",
    "            # delete additional space,if more than one space is present replace with single space\n",
    "            caption= caption.replace('\\s+', ' ')\n",
    "            # add start and end tags to the caption, remove single char/smaller words\n",
    "            caption = '<start> '+ ' '.join([word for word in caption.split() if len(word)>1]) + ' <end>'\n",
    "            \n",
    "            captions[i]= caption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# before preproces of text\n",
    "mapping['1000268201_693b08cb0e']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess the text\n",
    "clean_captions(mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After preprocessing\n",
    "mapping['1000268201_693b08cb0e']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create one single caption list\n",
    "all_captions= []\n",
    "for key in mapping:\n",
    "    for caption in mapping[key]:\n",
    "        all_captions.append(caption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(all_captions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_captions[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize the text\n",
    "tokenizer= Tokenizer()\n",
    "tokenizer.fit_on_texts(all_captions)\n",
    "\n",
    "vocab_size= len(tokenizer.word_index) + 1\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get max length of the caption available\n",
    "max_len= max(len(caption.split()) for caption in all_captions)\n",
    "max_len\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train test split\n",
    "image_ids= list(mapping.keys())\n",
    "split= int(len(image_ids) * 0.90)\n",
    "split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = image_ids[:split]\n",
    "test= image_ids[split:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create data generator to get data in batch (to avaoid session crash)\n",
    "def data_generator(data_keys, mapping, features, tokenizer, max_len, vocab_size, batch_size):\n",
    "    \n",
    "    # loop over images\n",
    "    X1, X2, y= [],[],[]\n",
    "    n= 0  # to chek if we reach the batch size or not\n",
    "    \n",
    "    while 1:\n",
    "        for key in data_keys:\n",
    "            n +=1\n",
    "            captions= mapping[key]\n",
    "            \n",
    "            # process each caption\n",
    "            for caption in captions:\n",
    "                # encode the seq\n",
    "                seq= tokenizer.texts_to_sequences([caption])[0]\n",
    "                # split the seq into X, y pairs\n",
    "                for i in range(1, len(seq)):\n",
    "                    # split into input and output pairs\n",
    "                    in_seq, out_seq= seq[:i], seq[i]\n",
    "                    # pad input seq\n",
    "                    in_seq= pad_sequences([in_seq], maxlen= max_len)[0]\n",
    "                    # encode output seq\n",
    "                    out_seq= to_categorical([out_seq], num_classes= vocab_size)[0]\n",
    "                    \n",
    "                    \n",
    "                    # store the seq \n",
    "                    X1.append(features[key][0])\n",
    "                    X2.append(in_seq)\n",
    "                    y.append(out_seq)\n",
    "                    \n",
    "            if n == batch_size:\n",
    "                X1, X2, y = np.array(X1), np.array(X2), np.array(y)\n",
    "                print(X1, X2, y)\n",
    "                # returns collected samples in the generator so that it will be consumed by the model\n",
    "#                 yield [X1, X2], y\n",
    "                X1, X2, y= [], [], []\n",
    "                n= 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_generator(train[:5], mapping, features, tokenizer, max_len, vocab_size, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder Model: \n",
    "\n",
    "#Image Feature Layers\n",
    "inputs1= Input(shape= (4096,))\n",
    "fe1= Dropout(0.4)(inputs1)\n",
    "fe2= Dense(256, activation= 'relu')(fe1)\n",
    "\n",
    "# sequence feature layer\n",
    "inputs2= Input(shape= ( max_len,))\n",
    "se1= Embedding(vocab_size, 256, mask_zero= True)(inputs2)\n",
    "se2= Dropout(0.4)(se1)\n",
    "se3= LSTM(256)(se2)\n",
    "\n",
    "# Decoder Model:\n",
    "\n",
    "# concat image and text features\n",
    "decoder1= add([fe2, se3])\n",
    "decoder2= Dense(256, activation= 'relu')(decoder1)\n",
    "outputs= Dense(vocab_size, activation= 'softmax')(decoder2)\n",
    "\n",
    "\n",
    "model= Model(inputs= [inputs1, inputs2], outputs= outputs)\n",
    "model.compile(loss= 'categorical_cross_entropy', optimizer= 'adam')\n",
    "\n",
    "# plot the model\n",
    "plot_model(model, show_shapes= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the model\n",
    "epochs= 15\n",
    "batch_size= 64\n",
    "# after each step it will do the back prop and woll fetch the data\n",
    "steps= len(train) // batch_size\n",
    "\n",
    "for i in range(epochs):\n",
    "    X1, X2, y= data_generator(train, mapping, features, tokenizer, max_len, vocab_size, batch_size)\n",
    "#     print(generator)\n",
    "    # fit for one epoch\n",
    "    model.fit([X1, X2], y, epochs= 1, steps_per_epoch= steps, verbose= 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "model.save(WORKING_DIR + '/best_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Captions for the image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "def idx_to_word(integer, tokenizer):\n",
    "    for word, idxin tokenizer.word_index.items():\n",
    "        if index == integer:\n",
    "            return word\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## generate caption for an image\n",
    "def predict_caption( model, image, tokenizer, max_len):\n",
    "    # add start for a generation process\n",
    "    in_text= '<start>'\n",
    "    # iterate over the max_len of sequence\n",
    "    for i in range(max_len):\n",
    "        # encode inout into seq\n",
    "        seq= tokenizer.text_to_sequences(in_text)[0]\n",
    "        # pad the seq\n",
    "        seq= pad_sequences([seq], maxlen)\n",
    "        # predict next word\n",
    "        ypred= model.predict([image, seq], verbose= 0)\n",
    "        # get index with high prob \n",
    "        ypred= np.argmax(ypred)\n",
    "        # convert index to word\n",
    "        word= idx_to_word( ypred, tokenizer)\n",
    "        # stop if word not found\n",
    "        if word in None:\n",
    "            break\n",
    "        # append word as input for generating next word\n",
    "        in_text += ' '+ word\n",
    "        # stop if we reach end tag\n",
    "        if word== '<end>':\n",
    "            break\n",
    "    return in_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " from nltk.translate.bleu_score import corpus_bleu\n",
    "# Validate with test data\n",
    "actual, predicted= [], []\n",
    "for key in tqdm(test):\n",
    "    # get actual caption\n",
    "    captions= mapping['key']\n",
    "    # predict the cation for mapping\n",
    "    y_pred= predict_caption(model, features[key], tokenizer, max_len)\n",
    "    actual_caption= [caption.split() for caption in captions] \n",
    "    y_pred= y_pred.split()\n",
    "    \n",
    "    # append to the list\n",
    "    actual.append(actual_caption)\n",
    "    predicted.append(y_pred)\n",
    "    \n",
    "# calculate BLEU score\n",
    "print('BLEU-1: %f'% corpus_bleu(actual, predicted, weights = [1.0, 0, 0, 0]))\n",
    "print('BLEU-2: %f'% corpus_bleu(actual, predicted, weights = [0.5, 0.5, 0, 0]))   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize the Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def generate_caption(image_name):\n",
    "    # load the image\n",
    "#     image_name=  '1001773457_577c3a7d70.jpg'\n",
    "    image_id= image_name.split('.')[0]\n",
    "    img_path= os.path.join(BASE_DIR, 'Images', image_name)\n",
    "    image= Image.open(img_path)\n",
    "    captions= mapping[image_id]\n",
    "    print('---------Actual ----------------')\n",
    "    for caption in captions:\n",
    "        print(captions)\n",
    "\n",
    "    # predict the caption\n",
    "    y_pred= predict_caption(model, features[image_id], tokenizer, max_len)\n",
    "    print('---------Predicted ----------------')\n",
    "    print(y_pred)\n",
    "    plt.imshow(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_caption('1001773457_577c3a7d70.jpg')"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 1371688,
     "sourceId": 2277386,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30674,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
