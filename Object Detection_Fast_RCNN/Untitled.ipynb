{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee7725e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6564fe44",
   "metadata": {},
   "source": [
    "## Fast-RCNN\n",
    "\n",
    "Fast R-CNN is an extension of the R-CNN (Region-based Convolutional Neural Network) object detection algorithm, which aims to address some of the limitations of the original R-CNN in terms of speed and efficiency. \n",
    "\n",
    "Fast R-CNN introduces several improvements to the R-CNN pipeline, including the use of a single network for both region proposal and object detection, as well as the use of a region of interest (RoI) pooling layer to efficiently extract features from proposed regions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fafe0f04",
   "metadata": {},
   "source": [
    "### Steps for Fast-RCNN:\n",
    "\n",
    "1. Preprocessing: Load and preprocess the input image.\n",
    "\n",
    "2. Feature Extraction: Pass the preprocessed image through a pre-trained CNN to extract feature maps.\n",
    "\n",
    "\n",
    "3. Region Proposal: Use a region proposal network (RPN) or an external method (e.g., selective search) to generate region proposals based on the feature maps.\n",
    "\n",
    "4. RoI Pooling: Apply RoI pooling to the feature maps to extract fixed-size features for each proposed region.\n",
    "\n",
    "5. Classification and Regression: Pass the RoI-pooled features through fully connected layers for object classification and bounding box regression.\n",
    "\n",
    "6. Non-Maximum Suppression: Apply non-maximum suppression to filter out redundant detections and produce the final set of object detections.\n",
    "\n",
    "7. Post-processing: Optionally, perform additional post-processing steps such as filtering detections based on confidence scores or applying additional heuristics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7a397e9",
   "metadata": {},
   "source": [
    "#### Implementing Fast-RCNN using Tensorflow Object Detection API\n",
    "\n",
    "Steps for Fast-RCNN implementation:\n",
    "\n",
    "1. Install TensorFlow Object Detection API: First, you need to install the TensorFlow Object Detection API by following the installation instructions provided in the official documentation.\n",
    "\n",
    "2. Download Pre-trained Model: Download a pre-trained Fast R-CNN model checkpoint from the TensorFlow Model Zoo. The Model Zoo provides pre-trained models for various object detection architectures, including Fast R-CNN.\n",
    "\n",
    "3. Set Up Pipeline Configuration: Configure the object detection pipeline by creating a pipeline configuration file. This file specifies parameters such as the model architecture, input image size, class labels, and paths to the pre-trained model checkpoint and label map.\n",
    "\n",
    "4. Load Pre-trained Model: Load the pre-trained Fast R-CNN model checkpoint and instantiate the object detection model using TensorFlow's Object Detection API.\n",
    "\n",
    "5. Perform Inference: Use the loaded model to perform inference on input images or video frames. The model will detect objects in the input images and classify them into predefined classes.\n",
    "\n",
    "6. Visualize Results: Visualize the detected objects and their bounding boxes on the input images or video frames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ff2c5caa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\varsha\\anaconda3\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import six.moves.urllib as urllib\n",
    "import sys\n",
    "import tarfile\n",
    "import tensorflow as tf\n",
    "import zipfile\n",
    "\n",
    "from collections import defaultdict\n",
    "from io import StringIO\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "# Import the Object Detection API modules\n",
    "from object_detection.utils import ops as utils_ops\n",
    "# from object_detection.utils import label_map_util\n",
    "from object_detection.utils import visualization_utils as vis_util\n",
    "from PIL import Image\n",
    "matplotlib.use('TkAgg')  # or 'Qt5Agg'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "14dc2ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(file_path):\n",
    "    \n",
    "    classLabels=[]\n",
    "    filename= file_path\n",
    "    \n",
    "    with open(filename,'rt') as fpt:\n",
    "        classLabels = fpt.read().rstrip('\\n').split('\\n')\n",
    "\n",
    "    return classLabels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "63f11c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_pbtxt_file(file_path):\n",
    "    \n",
    "    label_map = read_file(file_path)\n",
    "#     print(label_map)\n",
    "    file_name= 'label_map.pbtxt'\n",
    "    with open(file_name, 'w') as f:\n",
    "        for idx, class_name in enumerate(label_map):\n",
    "            f.write('item{\\n')\n",
    "            f.write(f'id:{idx}\\n')\n",
    "            f.write(f\"name:'{class_name}'\\n\")\n",
    "            f.write('}\\n')\n",
    "            \n",
    "    return file_name           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "84a0d3b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_label_map(file_path):\n",
    "    \n",
    "    label_map_path= create_pbtxt_file(file_path)\n",
    "    \n",
    "    label_map = {}\n",
    "    with open(label_map_path, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "        # Initialize variables to store ID and name\n",
    "        class_id = None\n",
    "        class_name = None\n",
    "        # Iterate through each line in the file\n",
    "        for line in lines:\n",
    "            # Check for the start of a new item block\n",
    "            if 'item {' in line:\n",
    "                # Reset class ID and name\n",
    "                class_id = None\n",
    "                class_name = None\n",
    "            # Check for ID line\n",
    "            elif 'id' in line:\n",
    "                # Extract class ID\n",
    "                class_id = (line.split(':')[1])\n",
    "            # Check for name line\n",
    "            elif 'name' in line:\n",
    "                # Extract class name\n",
    "                class_name = line.split(\":\")[1].strip()[1:-1]  # Remove quotes and leading/trailing whitespace\n",
    "                # Store ID and name in dictionary\n",
    "                if class_id is not None and class_name is not None:\n",
    "                    label_map[class_id] = class_name\n",
    "    return label_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f981d30a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PATH_TO_LABELS = r\"D:\\Transfer Learning Model\\Labels\\imagenet.shortnames.list\"\n",
    "\n",
    "# category_index = read_label_map(PATH_TO_LABELS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dbf9be54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# category_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1a2a9ca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the paths for the model and label map\n",
    "PATH_TO_MODEL_DIR = r\"D:\\Transfer Learning Model\\Faster RCNN\\faster_rcnn_inception_resnet_v2_640x640_coco17_tpu-8\\faster_rcnn_inception_resnet_v2_640x640_coco17_tpu-8\\saved_model\"\n",
    "PATH_TO_LABELS = r\"D:\\Transfer Learning Model\\Labels\\imagenet.shortnames.list\"\n",
    "\n",
    "# Load the model\n",
    "detection_model = tf.saved_model.load(PATH_TO_MODEL_DIR)\n",
    "\n",
    "# Load label map using the alternate method\n",
    "category_index = read_label_map(PATH_TO_LABELS)\n",
    "\n",
    "# Load pre-trained model\n",
    "detect_fn = tf.saved_model.load(PATH_TO_MODEL_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "143414fe",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'vis_utils' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 14\u001b[0m\n\u001b[0;32m      9\u001b[0m detections \u001b[38;5;241m=\u001b[39m detect_fn(input_tensor)\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# print(detections['detection_boxes'][0].numpy())\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# print(category_index)\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# Visualize results\u001b[39;00m\n\u001b[1;32m---> 14\u001b[0m vis_utils\u001b[38;5;241m.\u001b[39mvisualize_boxes_and_labels_on_image_array(\n\u001b[0;32m     15\u001b[0m     image_np\u001b[38;5;241m.\u001b[39mnumpy(),\n\u001b[0;32m     16\u001b[0m     detections[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdetection_boxes\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mnumpy(),\n\u001b[0;32m     17\u001b[0m     detections[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdetection_classes\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mint\u001b[39m),\n\u001b[0;32m     18\u001b[0m     detections[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdetection_scores\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mnumpy(),\n\u001b[0;32m     19\u001b[0m     category_index,\n\u001b[0;32m     20\u001b[0m     use_normalized_coordinates\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m     21\u001b[0m     max_boxes_to_draw\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m200\u001b[39m,\n\u001b[0;32m     22\u001b[0m     min_score_thresh\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m,\n\u001b[0;32m     23\u001b[0m     agnostic_mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m     24\u001b[0m )\n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m# Display the results\u001b[39;00m\n\u001b[0;32m     27\u001b[0m plt\u001b[38;5;241m.\u001b[39mimshow(image_np)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'vis_utils' is not defined"
     ]
    }
   ],
   "source": [
    "# Perform inference on input image\n",
    "image_path = r\"C:\\Users\\varsha\\Pictures\\CV_IMG\\1200.jpg\"\n",
    "image_np = tf.io.read_file(image_path)\n",
    "image_np = tf.image.decode_jpeg(image_np, channels=3)\n",
    "\n",
    "input_tensor = tf.convert_to_tensor(image_np)\n",
    "input_tensor = input_tensor[tf.newaxis, ...]\n",
    "\n",
    "detections = detect_fn(input_tensor)\n",
    "# print(detections['detection_boxes'][0].numpy())\n",
    "# print(category_index)\n",
    "\n",
    "# Visualize results\n",
    "vis_utils.visualize_boxes_and_labels_on_image_array(\n",
    "    image_np.numpy(),\n",
    "    detections['detection_boxes'][0].numpy(),\n",
    "    detections['detection_classes'][0].numpy().astype(int),\n",
    "    detections['detection_scores'][0].numpy(),\n",
    "    category_index,\n",
    "    use_normalized_coordinates=True,\n",
    "    max_boxes_to_draw=200,\n",
    "    min_score_thresh=0.5,\n",
    "    agnostic_mode=False\n",
    ")\n",
    "\n",
    "# Display the results\n",
    "plt.imshow(image_np)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "873af336",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d108a4f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Function to run inference on a single image\n",
    "# def run_inference_for_single_image(model, image):\n",
    "#     # Convert the image to a numpy array\n",
    "#     image_np = np.array(image)\n",
    "#     # Add a batch dimension to the image\n",
    "#     input_tensor = tf.convert_to_tensor(image_np)\n",
    "#     input_tensor = input_tensor[tf.newaxis, ...]\n",
    "#     # Run inference\n",
    "#     model_fn = model.signatures['serving_default']\n",
    "#     output_dict = model_fn(input_tensor)\n",
    "#     # Convert the model output tensors to numpy arrays\n",
    "#     num_detections = int(output_dict.pop('num_detections'))\n",
    "#     output_dict = {key: value[0, :num_detections].numpy() for key, value in output_dict.items()}\n",
    "#     output_dict['num_detections'] = num_detections\n",
    "#     # Handle models with masks\n",
    "#     if 'detection_masks' in output_dict:\n",
    "#         detection_masks_reframed = utils_ops.reframe_box_masks_to_image_masks(\n",
    "#             output_dict['detection_masks'], output_dict['detection_boxes'],\n",
    "#             image.shape[0], image.shape[1])\n",
    "#         detection_masks_reframed = tf.cast(detection_masks_reframed > 0.5,\n",
    "#                                            tf.uint8)\n",
    "#         output_dict['detection_masks_reframed'] = detection_masks_reframed.numpy()\n",
    "#     return output_dict\n",
    "\n",
    "# # Function to visualize the detection results\n",
    "# def visualize(image_np, output_dict):\n",
    "#     # Visualize the detection boxes and labels on the image\n",
    "#     vis_util.visualize_boxes_and_labels_on_image_array(\n",
    "#         image_np,\n",
    "#         output_dict['detection_boxes'],\n",
    "#         output_dict['detection_classes'],\n",
    "#         output_dict['detection_scores'],\n",
    "#         category_index,\n",
    "#         instance_masks=output_dict.get('detection_masks_reframed', None),\n",
    "#         use_normalized_coordinates=True,\n",
    "#         line_thickness=8)\n",
    "#     return image_np\n",
    "\n",
    "# # Load an image\n",
    "# PATH_TO_IMAGE = r\"C:\\Users\\Pictures\\CV_IMG\\1200.jpg\"\n",
    "# image = Image.open(PATH_TO_IMAGE)\n",
    "\n",
    "# # Run inference on the image\n",
    "# output_dict = run_inference_for_single_image(detection_model, image)\n",
    "\n",
    "# # Visualize the detection results\n",
    "# image_np = visualize(int(np.array(image)), output_dict)\n",
    "# plt.figure(figsize=(12, 8))\n",
    "# plt.imshow(image_np)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c60ae80a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the paths for the model and label map\n",
    "PATH_TO_MODEL_DIR = r\"D:\\Transfer Learning Model\\Faster RCNN\\faster_rcnn_inception_resnet_v2_640x640_coco17_tpu-8\\faster_rcnn_inception_resnet_v2_640x640_coco17_tpu-8\\saved_model\"\n",
    "PATH_TO_LABELS = r\"D:\\Transfer Learning Model\\Labels\\imagenet.shortnames.list\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2531a72c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from object_detection.utils import visualization_utils as vis_utils\n",
    "\n",
    "# Load the pre-trained model\n",
    "detect_fn = tf.saved_model.load(r\"D:\\Transfer Learning Model\\Faster RCNN\\faster_rcnn_inception_resnet_v2_640x640_coco17_tpu-8\\faster_rcnn_inception_resnet_v2_640x640_coco17_tpu-8\\saved_model\")\n",
    "\n",
    "# Load the ImageNet labels\n",
    "with open('imagenet.shortnames.list', 'r') as f:\n",
    "    class_names = f.read().splitlines()\n",
    "\n",
    "# Perform inference on input image\n",
    "image_path = r\"C:\\Users\\Pictures\\CV_IMG\\1200.jpg\"\n",
    "image_np = tf.io.read_file(image_path)\n",
    "image_np = tf.image.decode_jpeg(image_np, channels=3)\n",
    "\n",
    "input_tensor = tf.convert_to_tensor(image_np)\n",
    "input_tensor = input_tensor[tf.newaxis, ...]\n",
    "\n",
    "detections = detect_fn(input_tensor)\n",
    "\n",
    "# Visualize results\n",
    "vis_utils.visualize_boxes_and_labels_on_image_array(\n",
    "    image_np.numpy(),\n",
    "    detections['detection_boxes'][0].numpy(),\n",
    "    detections['detection_classes'][0].numpy().astype(int),\n",
    "    detections['detection_scores'][0].numpy(),\n",
    "    class_names,\n",
    "    use_normalized_coordinates=True,\n",
    "    max_boxes_to_draw=200,\n",
    "    min_score_thresh=0.5,\n",
    "    agnostic_mode=False\n",
    ")\n",
    "\n",
    "# Display the results\n",
    "plt.imshow(image_np)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3b0bb3a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1e055750a10>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "IMAGE_PATH= r\"C:\\Users\\varsha\\Pictures\\CV_IMG\\1200.jpg\"\n",
    "\n",
    "img= cv2.imread(r\"C:\\Users\\varsha\\Pictures\\CV_IMG\\1200.jpg\")\n",
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "114884bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_n= cv2.resize(img, (600,1024))\n",
    "img_n= img_n/255.0\n",
    "# img_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "365eb2c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.imshow(img_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73616aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rectangle(xy=(0.350221, 0.169153), width=0.524498, height=0.711739, angle=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83eab18e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_boxes_to_original_form(boxes, image_shape):\n",
    "    # boxes: Numpy array of shape (num_boxes, 4) containing normalized bounding box coordinates\n",
    "    # image_shape: Tuple containing the height and width of the original image\n",
    "    \n",
    "    height, width = image_shape\n",
    "    original_boxes = []\n",
    "    for box in boxes:\n",
    "        ymin, xmin, ymax, xmax = box\n",
    "        ymin = int(ymin * height)\n",
    "        xmin = int(xmin * width)\n",
    "        ymax = int(ymax * height)\n",
    "        xmax = int(xmax * width)\n",
    "        original_boxes.append([ymin, xmin, ymax, xmax])\n",
    "    return original_boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1c2c41e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "normalized_boxes = [[0.169153, 0.350221, 0.711739, 0.524498]]  # Example normalized bounding box coordinates\n",
    "image_shape = (600,600)  # Example original image shape (height, width)\n",
    "\n",
    "original_boxes = convert_boxes_to_original_form(normalized_boxes, image_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d11ba69b",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "371e7ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "imr=cv2.rectangle(img_n, (358,101,  427, 537), (255, 0, 0), thickness= 4)\n",
    "cv2.imshow('dsk', imr)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc860ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install tensorflow-object-detection-api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00778ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip uninstall object_detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14f917a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model checkpoint and pipeline configuration\n",
    "# model_checkpoint_path = PATH_TO_MODEL_DIR \n",
    "# pipeline_config_path = 'pipeline.config'\n",
    "\n",
    "# # Load label map\n",
    "# label_map_path = PATH_TO_LABELS\n",
    "# category_index = load_label_map(PATH_TO_LABELS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d01488f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88062704",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
