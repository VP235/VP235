{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b0208daa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\varsha\\anaconda3\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# lib to read files and do math operations\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# lib for visualization purpose\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# lib for image processing \n",
    "import cv2\n",
    "\n",
    "# lib for NN and CNN\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "from keras.layers import Dense, Conv2D, MaxPool2D, Input\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "# lib for system and file saving\n",
    "import os\n",
    "import pathlib\n",
    "import joblib\n",
    "\n",
    "# lib for ML\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# lib for evaluation\n",
    "from sklearn.metrics import classification_report, accuracy_score, precision_score, confusion_matrix\n",
    "\n",
    "# lib for pretrained models\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.applications import MobileNet\n",
    "from tensorflow.keras.applications import InceptionV3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c66f9cee",
   "metadata": {},
   "source": [
    "### RCNN (Region-based Convolutional Neural Network):\n",
    "\n",
    "It is a classic object detection framework that localizes objects in an image and classifies them into different categories. The RCNN pipeline consists of several key components:\n",
    "\n",
    "-> Region Proposal: Generate region proposals using selective search or another region proposal method. These proposals represent potential object locations in the image.\n",
    "\n",
    "-> Feature Extraction: Extract features from each region proposal using a pre-trained CNN (Convolutional Neural Network). Typically, the CNN is trained on a large dataset like ImageNet for generic feature extraction.\n",
    "\n",
    "-> Classification: Classify each region proposal into different object categories using a classifier. This step involves training a classifier (e.g., SVM, Softmax classifier) on top of the extracted features.\n",
    "\n",
    "-> Bounding Box Regression: Refine the bounding box coordinates of each region proposal to better fit the object using regression techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f9e54dd",
   "metadata": {},
   "source": [
    "#### Steps For RCNN:\n",
    "\n",
    "1. Choose a Pre-trained CNN: Select a pre-trained CNN (e.g., VGG, ResNet, or MobileNet) to extract features from region proposals. You can use models pretrained on large datasets like ImageNet.\n",
    "\n",
    "2. Generate Region Proposals: Use a region proposal method (e.g., selective search) to generate region proposals in the image. These proposals represent potential object locations.\n",
    "\n",
    "3. Extract Features: For each region proposal, extract features using the chosen CNN. Pass each region through the CNN and extract features from one of the intermediate layers.\n",
    "\n",
    "4. Classification and Bounding Box Regression: For each extracted feature vector, pass it through a classifier to classify the object category. Additionally, use a regression model to refine the bounding box coordinates of each region proposal.\n",
    "\n",
    "5. Non-maximum Suppression: Apply non-maximum suppression to remove redundant and overlapping bounding boxes.\n",
    "\n",
    "6. Post-processing: Optionally, you can perform post-processing steps such as filtering out low-confidence detections or applying a threshold to the confidence scores.\n",
    "\n",
    "7. Visualize Results: Draw bounding boxes around detected objects and display the classification results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "78a37410",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_annotations(dir_path):\n",
    "    # folder path where we have our annotations\n",
    "    annot_path= dir_path+'\\\\'+'_annotations.csv'\n",
    "\n",
    "    # check the directory\n",
    "    print(os.listdir(dir_path)[0:5])\n",
    "\n",
    "\n",
    "    # load the annotations\n",
    "    annotations= pd.read_csv(annot_path)\n",
    "    annotations.head()\n",
    "    \n",
    "    return annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7b38d15d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract and map images name with their respective classes using dict\n",
    "\n",
    "def get_classes_labels_dict(annot, class_col, img_col):\n",
    "    class_dict= {}\n",
    "    label_dict= {}\n",
    "\n",
    "    for idx, i in enumerate(annot[class_col].unique()):\n",
    "        class_dict[i]= annot[annot[class_col]==i][img_col]\n",
    "        label_dict[i]= idx\n",
    "        \n",
    "    return class_dict, label_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "547654d8",
   "metadata": {},
   "source": [
    "#### Why did we use block5_pool of VGG16 as output?\n",
    "\n",
    "The layers of a CNN capture increasingly abstract features as you move deeper into the network. Layers closer to the input capture low-level features like edges and textures, while layers deeper in the network capture higher-level features like object parts and whole objects. The \"block5_pool\" layer, being one of the deepest layers in VGG16, provides a good balance between high-level semantic features and spatial resolution.\n",
    "\n",
    "This can be beneficial for tasks like object detection, where spatial information about object locations is important for accurate localization.\n",
    "\n",
    "Computational Efficiency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "49d0eb23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pretrained_model(pretrain_m):\n",
    "    \n",
    "    if pretrain_m == 'VGG16':\n",
    "        \n",
    "        # load VGG16   -------- Note if shape not deined, it byu default takes (224,224,3) shape\n",
    "        base_model= VGG16(weights= 'imagenet', include_top= False)\n",
    "        model= Model(inputs= base_model.input, outputs= base_model.get_layer('block5_pool').output)\n",
    "        \n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ff272003",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to calulate roi manually using patches\n",
    "\n",
    "def get_roi_manual(patch, actual_bb): # [xmin, xmax, ymin, ymax]\n",
    "    \n",
    "    I_xmin, I_ymin= max(patch[0], actual_bb[0]), max(patch[2], actual_bb[2])\n",
    "    I_xmax, I_ymax= min(patch[1], actual_bb[1]), min(patch[3], actual_bb[3])\n",
    "    \n",
    "    # check if patch and actual_bb are not overlapping on eachother\n",
    "    if I_xmax > I_xmin or I_ymax > I_ymin:\n",
    "        \n",
    "        inter_area= 0\n",
    "    else:   \n",
    "        # calculate intersection area = inter_l * inter_b\n",
    "        inter_area= np.abs((I_xmax - I_xmin) * (I_ymax - I_ymin))  # l*b\n",
    "\n",
    "        # calculate area of the patch and actual_bb = l*b\n",
    "        area_patch= (patch[1] - patch[0]) * (patch[3] - patch[2])\n",
    "        area_actual_bb= (actual_bb[1] - actual_bb[0]) * (actual_bb[3] - actual_bb[2])\n",
    "    \n",
    "        # calculate area of union = A(patch) + A(actual_bb) - inter_area\n",
    "        union_area= (area_patch + area_actual_bb) - inter_area\n",
    "        \n",
    "        # calculate iou= inter_area/union_are\n",
    "        iou= inter_area/union_area\n",
    "        \n",
    "        \n",
    "    return iou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "79569a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_valid_proposal(rects, actual_bb, min_iou= 0.5 ):\n",
    "    \n",
    "    for region in rects:\n",
    "        print(region)\n",
    "        print(actual_bb)\n",
    "        # calculate iou with actual_bb\n",
    "        iou= get_roi_manual(region, actual_bb)\n",
    "        \n",
    "        if any(iou >= min_iou):\n",
    "            return region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dfc1e7a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to create region of propsal\n",
    "\n",
    "def generate_reg_of_proposal(image):\n",
    "#     print(image)\n",
    "    # Create a Selective Search segmentation object\n",
    "    ss = cv2.ximgproc.segmentation.createSelectiveSearchSegmentation()\n",
    "\n",
    "    # Set input image for Selective Search\n",
    "    ss.setBaseImage(image)\n",
    "\n",
    "    # Switch to fast mode (optional, but recommended)\n",
    "    ss.switchToSelectiveSearchFast()\n",
    "\n",
    "    # Perform selective search to generate region proposals\n",
    "    rects = ss.process()\n",
    "\n",
    "    # Convert rects to list of tuples (x, y, w, h)\n",
    "    region_proposals = [(int(x), int(y), int(x + w), int(y + h)) for (x, y, w, h) in rects]\n",
    "\n",
    "    return  region_proposals                        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f86ce5cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install opencv-contrib-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bf35c064",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_test(X, y, split_size):\n",
    "    \n",
    "    # transform list to numpy array\n",
    "    X= np.array(X)\n",
    "    y= np.array(y)\n",
    "    \n",
    "    # split the data in train and test\n",
    "    X_train, X_test, y_train, y_test= train_test_split(X, y, test_size= split_size, random_state= 0)\n",
    "    \n",
    "    \n",
    "    return (X_train, y_train), (X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb55e48e",
   "metadata": {},
   "source": [
    "### Training Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "de58c0a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['IMG_2274_jpeg_jpg.rf.2f319e949748145fb22dcb52bb325a0c.jpg', 'IMG_2275_jpeg_jpg.rf.66355520a49ba7fb7082052f7ca6fee0.jpg', 'IMG_2276_jpeg_jpg.rf.7411b1902c81bad8cdefd2cc4eb3a97b.jpg', 'IMG_2280_jpeg_jpg.rf.5abcce5be523f6507bbaf731dd671226.jpg', 'IMG_2282_jpeg_jpg.rf.510f3bc14c3e0aa378b192199d01cae6.jpg']\n"
     ]
    }
   ],
   "source": [
    "# set dir_path for train folder and annotations\n",
    "dir_path= r\"D:\\Datasets\\Aquarium Combined.v2-raw-1024.tensorflow\\train\"\n",
    "\n",
    "# get the annotations or labels csv\n",
    "annot= get_annotations(dir_path)\n",
    "\n",
    "# create dic for images and labels\n",
    "class_dict, labels_dict= get_classes_labels_dict(annot, 'class', 'filename')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1140c0fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>width</th>\n",
       "      <th>height</th>\n",
       "      <th>class</th>\n",
       "      <th>xmin</th>\n",
       "      <th>ymin</th>\n",
       "      <th>xmax</th>\n",
       "      <th>ymax</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>IMG_2541_jpeg_jpg.rf.fc997b87790e715d47ce1cc83...</td>\n",
       "      <td>768</td>\n",
       "      <td>1024</td>\n",
       "      <td>starfish</td>\n",
       "      <td>302</td>\n",
       "      <td>410</td>\n",
       "      <td>534</td>\n",
       "      <td>730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>IMG_8406_jpg.rf.fda4b68f345bda8047e7f15060f70e...</td>\n",
       "      <td>1024</td>\n",
       "      <td>768</td>\n",
       "      <td>shark</td>\n",
       "      <td>106</td>\n",
       "      <td>442</td>\n",
       "      <td>175</td>\n",
       "      <td>640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>IMG_8406_jpg.rf.fda4b68f345bda8047e7f15060f70e...</td>\n",
       "      <td>1024</td>\n",
       "      <td>768</td>\n",
       "      <td>fish</td>\n",
       "      <td>638</td>\n",
       "      <td>490</td>\n",
       "      <td>678</td>\n",
       "      <td>672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>IMG_8406_jpg.rf.fda4b68f345bda8047e7f15060f70e...</td>\n",
       "      <td>1024</td>\n",
       "      <td>768</td>\n",
       "      <td>fish</td>\n",
       "      <td>625</td>\n",
       "      <td>107</td>\n",
       "      <td>765</td>\n",
       "      <td>187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>IMG_8406_jpg.rf.fda4b68f345bda8047e7f15060f70e...</td>\n",
       "      <td>1024</td>\n",
       "      <td>768</td>\n",
       "      <td>fish</td>\n",
       "      <td>818</td>\n",
       "      <td>419</td>\n",
       "      <td>830</td>\n",
       "      <td>499</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            filename  width  height     class  \\\n",
       "0  IMG_2541_jpeg_jpg.rf.fc997b87790e715d47ce1cc83...    768    1024  starfish   \n",
       "1  IMG_8406_jpg.rf.fda4b68f345bda8047e7f15060f70e...   1024     768     shark   \n",
       "2  IMG_8406_jpg.rf.fda4b68f345bda8047e7f15060f70e...   1024     768      fish   \n",
       "3  IMG_8406_jpg.rf.fda4b68f345bda8047e7f15060f70e...   1024     768      fish   \n",
       "4  IMG_8406_jpg.rf.fda4b68f345bda8047e7f15060f70e...   1024     768      fish   \n",
       "\n",
       "   xmin  ymin  xmax  ymax  \n",
       "0   302   410   534   730  \n",
       "1   106   442   175   640  \n",
       "2   638   490   678   672  \n",
       "3   625   107   765   187  \n",
       "4   818   419   830   499  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "annot.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "982f9221",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "115\n",
      "[[[ 43  33  33]\n",
      "  [ 47  37  37]\n",
      "  [ 48  38  38]\n",
      "  ...\n",
      "  [101 106 115]\n",
      "  [ 80  84  95]\n",
      "  [ 67  71  82]]\n",
      "\n",
      " [[ 45  35  35]\n",
      "  [ 47  37  37]\n",
      "  [ 47  37  37]\n",
      "  ...\n",
      "  [ 83  88  97]\n",
      "  [ 80  84  95]\n",
      "  [ 82  86  97]]\n",
      "\n",
      " [[ 41  31  31]\n",
      "  [ 40  30  30]\n",
      "  [ 40  30  30]\n",
      "  ...\n",
      "  [ 67  69  80]\n",
      "  [ 59  61  72]\n",
      "  [ 61  63  74]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[ 88  95 144]\n",
      "  [104 111 160]\n",
      "  [108 112 160]\n",
      "  ...\n",
      "  [  1   1   1]\n",
      "  [  1   1   1]\n",
      "  [  1   1   1]]\n",
      "\n",
      " [[ 85  91 138]\n",
      "  [ 95 101 146]\n",
      "  [ 96  99 143]\n",
      "  ...\n",
      "  [  1   1   1]\n",
      "  [  1   1   1]\n",
      "  [  1   1   1]]\n",
      "\n",
      " [[ 88  94 139]\n",
      "  [ 92  98 141]\n",
      "  [ 94  95 139]\n",
      "  ...\n",
      "  [  1   1   1]\n",
      "  [  1   1   1]\n",
      "  [  1   1   1]]]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'int' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 28\u001b[0m\n\u001b[0;32m     22\u001b[0m regions\u001b[38;5;241m=\u001b[39m generate_reg_of_proposal(image)\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m region \u001b[38;5;129;01min\u001b[39;00m regions:\n\u001b[0;32m     25\u001b[0m \n\u001b[0;32m     26\u001b[0m     \u001b[38;5;66;03m# run the region through iou to get the only those regions which has \u001b[39;00m\n\u001b[0;32m     27\u001b[0m     \u001b[38;5;66;03m# high iou overlap  with the actual_bb--------(extra step added by me)\u001b[39;00m\n\u001b[1;32m---> 28\u001b[0m     region \u001b[38;5;241m=\u001b[39m get_valid_proposal(region, image)\n\u001b[0;32m     30\u001b[0m     \u001b[38;5;66;03m# set the extracted coordinates \u001b[39;00m\n\u001b[0;32m     31\u001b[0m     (x, y, w, h)\u001b[38;5;241m=\u001b[39m region\n",
      "Cell \u001b[1;32mIn[15], line 7\u001b[0m, in \u001b[0;36mget_valid_proposal\u001b[1;34m(rects, actual_bb, min_iou)\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(actual_bb)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# calculate iou with actual_bb\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m iou\u001b[38;5;241m=\u001b[39m get_roi_manual(region, actual_bb)\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(iou \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m min_iou):\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m region\n",
      "Cell \u001b[1;32mIn[5], line 5\u001b[0m, in \u001b[0;36mget_roi_manual\u001b[1;34m(patch, actual_bb)\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_roi_manual\u001b[39m(patch, actual_bb): \u001b[38;5;66;03m# [xmin, xmax, ymin, ymax]\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m     I_xmin, I_ymin\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(patch[\u001b[38;5;241m0\u001b[39m], actual_bb[\u001b[38;5;241m0\u001b[39m]), \u001b[38;5;28mmax\u001b[39m(patch[\u001b[38;5;241m2\u001b[39m], actual_bb[\u001b[38;5;241m2\u001b[39m])\n\u001b[0;32m      6\u001b[0m     I_xmax, I_ymax\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(patch[\u001b[38;5;241m1\u001b[39m], actual_bb[\u001b[38;5;241m1\u001b[39m]), \u001b[38;5;28mmin\u001b[39m(patch[\u001b[38;5;241m3\u001b[39m], actual_bb[\u001b[38;5;241m3\u001b[39m])\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;66;03m# check if patch and actual_bb are not overlapping on eachother\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'int' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "# load pre-trained model\n",
    "model = get_pretrained_model('VGG16')\n",
    "\n",
    "# load classifier (SVM or Softmax)\n",
    "classifier= SVC()\n",
    "\n",
    "# extract features and labels\n",
    "features_train= []\n",
    "labels_train= []\n",
    "\n",
    "for i, row in annot.iterrows():\n",
    "    \n",
    "#     print(row)\n",
    "    actual_bb= [row.xmin, row.xmax, row.ymin, row.ymax]\n",
    "    image= cv2.imread(dir_path+ '\\\\' +row['filename'])\n",
    "\n",
    "    if image is None:\n",
    "        print('wrong path !!', dir_path, image)\n",
    "\n",
    "    else:\n",
    "\n",
    "        regions= generate_reg_of_proposal(image)\n",
    "\n",
    "        for region in regions:\n",
    "\n",
    "            # run the region through iou to get the only those regions which has \n",
    "            # high iou overlap  with the actual_bb--------(extra step added by me)\n",
    "            region = get_valid_proposal(region, image)\n",
    "\n",
    "            # set the extracted coordinates \n",
    "            (x, y, w, h)= region\n",
    "\n",
    "            # crop the image to get the region of interest\n",
    "            roi= image[y:y+h, x:x+w]\n",
    "\n",
    "            # resize the roi image to (224,224) to fit VGG16 input\n",
    "            roi_resized= cv2.resize(roi, (224, 224))\n",
    "            roi_preprocessed= np.expand_dims(cv2.cvtColor(roi_resized, cv2.COLOR_BGR2RGB), axis=0)\n",
    "\n",
    "            # extract features\n",
    "            features= model.predict(roi_preprocessed)\n",
    "            feature_flatten= features.flatten()\n",
    "\n",
    "            # append features and labels to the list\n",
    "            features_train.append(features)\n",
    "            labels_train.append(row['class'])\n",
    "\n",
    "# Train the classifier\n",
    "classifier.fit(features_train, labels_train)\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26f812ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for row in annot.iterrows():\n",
    "    \n",
    "#     print(row)\n",
    "#     print(row)\n",
    "for i, rows in annot.iterrows():\n",
    "    print(rows['filename'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebbba2fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "img= cv2.imread(r\"C:\\Users\\varsha\\Pictures\\AI-Snaps\\1200.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8cb098c",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = cv2.imread(r\"D:\\Datasets\\Aquarium Combined.v2-raw-1024.tensorflow\\train\\IMG_2284_jpeg_jpg.rf.99de11cb5727748bd3eae3afe7b415e6.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcacc4b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Selective Search segmentation object\n",
    "ss = cv2.ximgproc.segmentation.createSelectiveSearchSegmentation()\n",
    "\n",
    "# Set input image for Selective Search\n",
    "ss.setBaseImage(i)\n",
    "\n",
    "# Switch to fast mode (optional, but recommended)\n",
    "ss.switchToSelectiveSearchFast()\n",
    "\n",
    "# Perform selective search to generate region proposals\n",
    "rects = ss.process()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6900f538",
   "metadata": {},
   "outputs": [],
   "source": [
    "rects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55af48e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for region in region_proposals:\n",
    "#     print(region)\n",
    "    (x, y, w, h)= region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f158737",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a499edd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94b9fa75",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "745df83a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98dea8c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0709f985",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e27e15f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e038a60c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e04c6e53",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
