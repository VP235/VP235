{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b850f6a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\varsha\\anaconda3\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "from keras.layers import Dense, Embedding, Input, LSTM, Bidirectional\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, precision_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import sent_tokenize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8b49efc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df= pd.read_csv(r\"D:\\Datasets\\medium_data_Bidirectional.csv\\medium_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ae237ee0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>url</th>\n",
       "      <th>title</th>\n",
       "      <th>subtitle</th>\n",
       "      <th>image</th>\n",
       "      <th>claps</th>\n",
       "      <th>responses</th>\n",
       "      <th>reading_time</th>\n",
       "      <th>publication</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>https://towardsdatascience.com/a-beginners-gui...</td>\n",
       "      <td>A Beginner’s Guide to Word Embedding with Gens...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.png</td>\n",
       "      <td>850</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>Towards Data Science</td>\n",
       "      <td>2019-05-30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>https://towardsdatascience.com/hands-on-graph-...</td>\n",
       "      <td>Hands-on Graph Neural Networks with PyTorch &amp; ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.png</td>\n",
       "      <td>1100</td>\n",
       "      <td>11</td>\n",
       "      <td>9</td>\n",
       "      <td>Towards Data Science</td>\n",
       "      <td>2019-05-30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>https://towardsdatascience.com/how-to-use-ggpl...</td>\n",
       "      <td>How to Use ggplot2 in Python</td>\n",
       "      <td>A Grammar of Graphics for Python</td>\n",
       "      <td>3.png</td>\n",
       "      <td>767</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>Towards Data Science</td>\n",
       "      <td>2019-05-30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>https://towardsdatascience.com/databricks-how-...</td>\n",
       "      <td>Databricks: How to Save Files in CSV on Your L...</td>\n",
       "      <td>When I work on Python projects dealing…</td>\n",
       "      <td>4.jpeg</td>\n",
       "      <td>354</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>Towards Data Science</td>\n",
       "      <td>2019-05-30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>https://towardsdatascience.com/a-step-by-step-...</td>\n",
       "      <td>A Step-by-Step Implementation of Gradient Desc...</td>\n",
       "      <td>One example of building neural…</td>\n",
       "      <td>5.jpeg</td>\n",
       "      <td>211</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>Towards Data Science</td>\n",
       "      <td>2019-05-30</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                                url  \\\n",
       "0   1  https://towardsdatascience.com/a-beginners-gui...   \n",
       "1   2  https://towardsdatascience.com/hands-on-graph-...   \n",
       "2   3  https://towardsdatascience.com/how-to-use-ggpl...   \n",
       "3   4  https://towardsdatascience.com/databricks-how-...   \n",
       "4   5  https://towardsdatascience.com/a-step-by-step-...   \n",
       "\n",
       "                                               title  \\\n",
       "0  A Beginner’s Guide to Word Embedding with Gens...   \n",
       "1  Hands-on Graph Neural Networks with PyTorch & ...   \n",
       "2                       How to Use ggplot2 in Python   \n",
       "3  Databricks: How to Save Files in CSV on Your L...   \n",
       "4  A Step-by-Step Implementation of Gradient Desc...   \n",
       "\n",
       "                                  subtitle   image  claps responses  \\\n",
       "0                                      NaN   1.png    850         8   \n",
       "1                                      NaN   2.png   1100        11   \n",
       "2         A Grammar of Graphics for Python   3.png    767         1   \n",
       "3  When I work on Python projects dealing…  4.jpeg    354         0   \n",
       "4          One example of building neural…  5.jpeg    211         3   \n",
       "\n",
       "   reading_time           publication        date  \n",
       "0             8  Towards Data Science  2019-05-30  \n",
       "1             9  Towards Data Science  2019-05-30  \n",
       "2             5  Towards Data Science  2019-05-30  \n",
       "3             4  Towards Data Science  2019-05-30  \n",
       "4             4  Towards Data Science  2019-05-30  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1d089fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import PIL\n",
    "# import urllib.request\n",
    "# img= np.array(PIL.Image.open(urllib.request.urlopen(df['url'][0]+'/'+df['image'][0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0dbf8a46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6508, 10)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7562a596",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    Hands-on Graph Neural Networks with PyTorch & ...\n",
       "2                         How to Use ggplot2 in Python\n",
       "3    Databricks: How to Save Files in CSV on Your L...\n",
       "4    A Step-by-Step Implementation of Gradient Desc...\n",
       "5      An Easy Introduction to SQL for Data Scientists\n",
       "6                        Hypothesis testing visualized\n",
       "7    Introduction to Latent Matrix Factorization Re...\n",
       "8         Which 2020 Candidate is the Best at Twitter?\n",
       "9            What if AI model understanding were easy?\n",
       "Name: title, dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# out of all these columns we are intrested in only title column \n",
    "df['title'][1:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1da4a36f",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8e8683db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove unwanted character and words in the title\n",
    "\n",
    "# lower the words\n",
    "df['clean_data']= [i.lower() for i in df['title']]\n",
    "\n",
    "# remove punctuations \n",
    "df['clean_data']= [i.translate(str.maketrans(string.punctuation, ' ' * len(string.punctuation))) for i in df['clean_data']]\n",
    "\n",
    "# replace space and escape characters\n",
    "df['clean_data']= df['clean_data'].apply(lambda x: x.replace('\\xa0', ' '))\n",
    "df['clean_data']= df['clean_data'].apply(lambda x: x.replace('\\u200a', ' '))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6e752730",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>url</th>\n",
       "      <th>title</th>\n",
       "      <th>subtitle</th>\n",
       "      <th>image</th>\n",
       "      <th>claps</th>\n",
       "      <th>responses</th>\n",
       "      <th>reading_time</th>\n",
       "      <th>publication</th>\n",
       "      <th>date</th>\n",
       "      <th>clean_data</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>https://towardsdatascience.com/a-beginners-gui...</td>\n",
       "      <td>A Beginner’s Guide to Word Embedding with Gens...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.png</td>\n",
       "      <td>850</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>Towards Data Science</td>\n",
       "      <td>2019-05-30</td>\n",
       "      <td>a beginner’s guide to word embedding with gens...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>https://towardsdatascience.com/hands-on-graph-...</td>\n",
       "      <td>Hands-on Graph Neural Networks with PyTorch &amp; ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.png</td>\n",
       "      <td>1100</td>\n",
       "      <td>11</td>\n",
       "      <td>9</td>\n",
       "      <td>Towards Data Science</td>\n",
       "      <td>2019-05-30</td>\n",
       "      <td>hands on graph neural networks with pytorch   ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>https://towardsdatascience.com/how-to-use-ggpl...</td>\n",
       "      <td>How to Use ggplot2 in Python</td>\n",
       "      <td>A Grammar of Graphics for Python</td>\n",
       "      <td>3.png</td>\n",
       "      <td>767</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>Towards Data Science</td>\n",
       "      <td>2019-05-30</td>\n",
       "      <td>how to use ggplot2 in python</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>https://towardsdatascience.com/databricks-how-...</td>\n",
       "      <td>Databricks: How to Save Files in CSV on Your L...</td>\n",
       "      <td>When I work on Python projects dealing…</td>\n",
       "      <td>4.jpeg</td>\n",
       "      <td>354</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>Towards Data Science</td>\n",
       "      <td>2019-05-30</td>\n",
       "      <td>databricks  how to save files in csv on your l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>https://towardsdatascience.com/a-step-by-step-...</td>\n",
       "      <td>A Step-by-Step Implementation of Gradient Desc...</td>\n",
       "      <td>One example of building neural…</td>\n",
       "      <td>5.jpeg</td>\n",
       "      <td>211</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>Towards Data Science</td>\n",
       "      <td>2019-05-30</td>\n",
       "      <td>a step by step implementation of gradient desc...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                                url  \\\n",
       "0   1  https://towardsdatascience.com/a-beginners-gui...   \n",
       "1   2  https://towardsdatascience.com/hands-on-graph-...   \n",
       "2   3  https://towardsdatascience.com/how-to-use-ggpl...   \n",
       "3   4  https://towardsdatascience.com/databricks-how-...   \n",
       "4   5  https://towardsdatascience.com/a-step-by-step-...   \n",
       "\n",
       "                                               title  \\\n",
       "0  A Beginner’s Guide to Word Embedding with Gens...   \n",
       "1  Hands-on Graph Neural Networks with PyTorch & ...   \n",
       "2                       How to Use ggplot2 in Python   \n",
       "3  Databricks: How to Save Files in CSV on Your L...   \n",
       "4  A Step-by-Step Implementation of Gradient Desc...   \n",
       "\n",
       "                                  subtitle   image  claps responses  \\\n",
       "0                                      NaN   1.png    850         8   \n",
       "1                                      NaN   2.png   1100        11   \n",
       "2         A Grammar of Graphics for Python   3.png    767         1   \n",
       "3  When I work on Python projects dealing…  4.jpeg    354         0   \n",
       "4          One example of building neural…  5.jpeg    211         3   \n",
       "\n",
       "   reading_time           publication        date  \\\n",
       "0             8  Towards Data Science  2019-05-30   \n",
       "1             9  Towards Data Science  2019-05-30   \n",
       "2             5  Towards Data Science  2019-05-30   \n",
       "3             4  Towards Data Science  2019-05-30   \n",
       "4             4  Towards Data Science  2019-05-30   \n",
       "\n",
       "                                          clean_data  \n",
       "0  a beginner’s guide to word embedding with gens...  \n",
       "1  hands on graph neural networks with pytorch   ...  \n",
       "2                       how to use ggplot2 in python  \n",
       "3  databricks  how to save files in csv on your l...  \n",
       "4  a step by step implementation of gradient desc...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "27482c25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Learn From My Mistake. NEVER Delete Your\\xa0Website!'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['title'][124]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c1fb941d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'learn from my mistake  never delete your website '"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['clean_data'][124]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e79ae46",
   "metadata": {},
   "source": [
    "### Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "993ca46f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total words:  8233\n",
      "Word: ID\n",
      "---------------------------------\n",
      "strong:  3\n",
      "And:  7\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer() # For those words which are not found in word_index\n",
    "tokenizer.fit_on_texts(df['clean_data'])\n",
    "\n",
    "total_words= len(tokenizer.word_index) + 1\n",
    "\n",
    "print('Total words: ',total_words)\n",
    "print('Word: ID')\n",
    "print('---------------------------------')\n",
    "# print('<oov>: ', tokenizer.word_index['<oov>'])\n",
    "print('strong: ', tokenizer.word_index['strong'])\n",
    "print('And: ', tokenizer.word_index['and'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b8dca22f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'to': 1,\n",
       " 'the': 2,\n",
       " 'strong': 3,\n",
       " 'a': 4,\n",
       " 'of': 5,\n",
       " 'how': 6,\n",
       " 'and': 7,\n",
       " 'in': 8,\n",
       " 'your': 9,\n",
       " 'markup': 10,\n",
       " 'for': 11,\n",
       " 'you': 12,\n",
       " 'with': 13,\n",
       " 'is': 14,\n",
       " '—': 15,\n",
       " 'data': 16,\n",
       " 'why': 17,\n",
       " 'class': 18,\n",
       " 'h3': 19,\n",
       " 'i': 20,\n",
       " 'what': 21,\n",
       " 'on': 22,\n",
       " 'learning': 23,\n",
       " 'from': 24,\n",
       " 'an': 25,\n",
       " 'be': 26,\n",
       " 'my': 27,\n",
       " 'writing': 28,\n",
       " 'are': 29,\n",
       " 'it': 30,\n",
       " 'can': 31,\n",
       " 'using': 32,\n",
       " 'design': 33,\n",
       " 'machine': 34,\n",
       " 'ux': 35,\n",
       " 'about': 36,\n",
       " 'do': 37,\n",
       " 'not': 38,\n",
       " 'python': 39,\n",
       " 'ai': 40,\n",
       " 'life': 41,\n",
       " 'that': 42,\n",
       " 'when': 43,\n",
       " 'should': 44,\n",
       " 'we': 45,\n",
       " '5': 46,\n",
       " 'science': 47,\n",
       " 'make': 48,\n",
       " 'time': 49,\n",
       " 'need': 50,\n",
       " 'as': 51,\n",
       " '3': 52,\n",
       " 'more': 53,\n",
       " 'at': 54,\n",
       " 'business': 55,\n",
       " 'or': 56,\n",
       " 'part': 57,\n",
       " 'have': 58,\n",
       " 'work': 59,\n",
       " 'new': 60,\n",
       " 'don’t': 61,\n",
       " 'up': 62,\n",
       " 'by': 63,\n",
       " 'write': 64,\n",
       " 'get': 65,\n",
       " 'use': 66,\n",
       " 'guide': 67,\n",
       " 'will': 68,\n",
       " 'marketing': 69,\n",
       " '1': 70,\n",
       " 'ways': 71,\n",
       " 'deep': 72,\n",
       " 'best': 73,\n",
       " 'analysis': 74,\n",
       " 'first': 75,\n",
       " '2019': 76,\n",
       " 'product': 77,\n",
       " 'better': 78,\n",
       " 'things': 79,\n",
       " 'neural': 80,\n",
       " 'know': 81,\n",
       " 'this': 82,\n",
       " 'future': 83,\n",
       " 'good': 84,\n",
       " 'tips': 85,\n",
       " 'case': 86,\n",
       " 'one': 87,\n",
       " 'self': 88,\n",
       " 'into': 89,\n",
       " 'me': 90,\n",
       " '2': 91,\n",
       " 'if': 92,\n",
       " 'all': 93,\n",
       " 'out': 94,\n",
       " 'like': 95,\n",
       " 'top': 96,\n",
       " 'stop': 97,\n",
       " 'model': 98,\n",
       " 'intelligence': 99,\n",
       " 'job': 100,\n",
       " '10': 101,\n",
       " 'networks': 102,\n",
       " 'learned': 103,\n",
       " 'building': 104,\n",
       " 'artificial': 105,\n",
       " 'em': 106,\n",
       " 'big': 107,\n",
       " 'simple': 108,\n",
       " 'create': 109,\n",
       " 'being': 110,\n",
       " 'startup': 111,\n",
       " 'most': 112,\n",
       " 'way': 113,\n",
       " 'people': 114,\n",
       " 'build': 115,\n",
       " 'our': 116,\n",
       " 'vs': 117,\n",
       " 'learn': 118,\n",
       " 'introduction': 119,\n",
       " 'making': 120,\n",
       " 'market': 121,\n",
       " 'real': 122,\n",
       " 'want': 123,\n",
       " 'google': 124,\n",
       " 'social': 125,\n",
       " 'success': 126,\n",
       " 'day': 127,\n",
       " 'study': 128,\n",
       " 'story': 129,\n",
       " 'lessons': 130,\n",
       " 'steps': 131,\n",
       " 'it’s': 132,\n",
       " 'user': 133,\n",
       " 'right': 134,\n",
       " 'money': 135,\n",
       " '4': 136,\n",
       " 'writer': 137,\n",
       " 'app': 138,\n",
       " 'great': 139,\n",
       " 'content': 140,\n",
       " 'power': 141,\n",
       " 'models': 142,\n",
       " 'no': 143,\n",
       " 'world': 144,\n",
       " 'medium': 145,\n",
       " 'so': 146,\n",
       " 'you’re': 147,\n",
       " 'here’s': 148,\n",
       " '6': 149,\n",
       " 'start': 150,\n",
       " 'reasons': 151,\n",
       " 'help': 152,\n",
       " 'digital': 153,\n",
       " 'getting': 154,\n",
       " 'tech': 155,\n",
       " '7': 156,\n",
       " 'web': 157,\n",
       " 'blockchain': 158,\n",
       " 'own': 159,\n",
       " 'every': 160,\n",
       " 'just': 161,\n",
       " 'their': 162,\n",
       " 'network': 163,\n",
       " 'does': 164,\n",
       " 'career': 165,\n",
       " 'art': 166,\n",
       " 'media': 167,\n",
       " 'next': 168,\n",
       " 'than': 169,\n",
       " 'without': 170,\n",
       " 'year': 171,\n",
       " 'find': 172,\n",
       " 'who': 173,\n",
       " 'understanding': 174,\n",
       " 'code': 175,\n",
       " 'yourself': 176,\n",
       " 'designer': 177,\n",
       " 'change': 178,\n",
       " 'creating': 179,\n",
       " 'regression': 180,\n",
       " 'classification': 181,\n",
       " 'but': 182,\n",
       " 'project': 183,\n",
       " 'development': 184,\n",
       " 'tensorflow': 185,\n",
       " 'us': 186,\n",
       " 'book': 187,\n",
       " 'before': 188,\n",
       " 'detection': 189,\n",
       " 'writers': 190,\n",
       " 'step': 191,\n",
       " 'experience': 192,\n",
       " 'value': 193,\n",
       " 'now': 194,\n",
       " 'financial': 195,\n",
       " 'easy': 196,\n",
       " 'three': 197,\n",
       " 'isn’t': 198,\n",
       " 'through': 199,\n",
       " 'bad': 200,\n",
       " 'software': 201,\n",
       " 'made': 202,\n",
       " 'aws': 203,\n",
       " 'what’s': 204,\n",
       " 'designing': 205,\n",
       " 'customer': 206,\n",
       " 'image': 207,\n",
       " 'look': 208,\n",
       " 'never': 209,\n",
       " 'key': 210,\n",
       " 'really': 211,\n",
       " 'become': 212,\n",
       " 'them': 213,\n",
       " 'read': 214,\n",
       " 'team': 215,\n",
       " 'much': 216,\n",
       " 'strategy': 217,\n",
       " 'problem': 218,\n",
       " 'language': 219,\n",
       " 'love': 220,\n",
       " 'computer': 221,\n",
       " '2020': 222,\n",
       " 'technology': 223,\n",
       " 'important': 224,\n",
       " 'working': 225,\n",
       " 'online': 226,\n",
       " 'productivity': 227,\n",
       " 'improve': 228,\n",
       " 'between': 229,\n",
       " 'started': 230,\n",
       " 'too': 231,\n",
       " 'wrong': 232,\n",
       " 'process': 233,\n",
       " 'could': 234,\n",
       " 'based': 235,\n",
       " 'react': 236,\n",
       " 'company': 237,\n",
       " 'research': 238,\n",
       " 'home': 239,\n",
       " 'series': 240,\n",
       " 'its': 241,\n",
       " 'keep': 242,\n",
       " 'system': 243,\n",
       " 'ideas': 244,\n",
       " 'human': 245,\n",
       " 'api': 246,\n",
       " 'has': 247,\n",
       " 'startups': 248,\n",
       " 'go': 249,\n",
       " 'driven': 250,\n",
       " 'predicting': 251,\n",
       " 'ui': 252,\n",
       " 'designers': 253,\n",
       " 'application': 254,\n",
       " 'cloud': 255,\n",
       " 'less': 256,\n",
       " 'think': 257,\n",
       " 'text': 258,\n",
       " 'instagram': 259,\n",
       " 'creative': 260,\n",
       " 'nlp': 261,\n",
       " 'brand': 262,\n",
       " 'strategies': 263,\n",
       " '0': 264,\n",
       " 'save': 265,\n",
       " 'ml': 266,\n",
       " 'approach': 267,\n",
       " 'where': 268,\n",
       " 'free': 269,\n",
       " 'control': 270,\n",
       " 'productive': 271,\n",
       " 'doesn’t': 272,\n",
       " 'two': 273,\n",
       " 'r': 274,\n",
       " 'game': 275,\n",
       " 'take': 276,\n",
       " 'everything': 277,\n",
       " 'freelance': 278,\n",
       " 'writer’s': 279,\n",
       " 'needs': 280,\n",
       " 'leadership': 281,\n",
       " 'i’m': 282,\n",
       " 'algorithms': 283,\n",
       " 'was': 284,\n",
       " 'tell': 285,\n",
       " 'industry': 286,\n",
       " 'review': 287,\n",
       " 'linear': 288,\n",
       " 'trust': 289,\n",
       " 'brain': 290,\n",
       " 'mobile': 291,\n",
       " 'five': 292,\n",
       " 'let’s': 293,\n",
       " 'stories': 294,\n",
       " 'testing': 295,\n",
       " 'successful': 296,\n",
       " 'after': 297,\n",
       " 'here': 298,\n",
       " 'living': 299,\n",
       " 'over': 300,\n",
       " 'facebook': 301,\n",
       " 'space': 302,\n",
       " 'avoid': 303,\n",
       " 'house': 304,\n",
       " 'end': 305,\n",
       " 'mind': 306,\n",
       " 'thinking': 307,\n",
       " 'open': 308,\n",
       " 'tools': 309,\n",
       " 'companies': 310,\n",
       " 'systems': 311,\n",
       " 'down': 312,\n",
       " 'android': 313,\n",
       " 'javascript': 314,\n",
       " 'they': 315,\n",
       " 'habits': 316,\n",
       " 'finding': 317,\n",
       " 'changing': 318,\n",
       " 'feature': 319,\n",
       " 'back': 320,\n",
       " 'skills': 321,\n",
       " 'i’ve': 322,\n",
       " 'effective': 323,\n",
       " 'advice': 324,\n",
       " 'pandas': 325,\n",
       " 'scientists': 326,\n",
       " 'interview': 327,\n",
       " 'questions': 328,\n",
       " 'books': 329,\n",
       " 'years': 330,\n",
       " 'talk': 331,\n",
       " 'break': 332,\n",
       " 'reinforcement': 333,\n",
       " 'state': 334,\n",
       " 'growth': 335,\n",
       " 'reality': 336,\n",
       " 'enough': 337,\n",
       " 'importance': 338,\n",
       " 'search': 339,\n",
       " 'got': 340,\n",
       " 'these': 341,\n",
       " 'ask': 342,\n",
       " 'recognition': 343,\n",
       " 'pytorch': 344,\n",
       " 'management': 345,\n",
       " 'starting': 346,\n",
       " 'high': 347,\n",
       " 'privacy': 348,\n",
       " 'must': 349,\n",
       " 'can’t': 350,\n",
       " 'always': 351,\n",
       " 'might': 352,\n",
       " 'side': 353,\n",
       " 'mistakes': 354,\n",
       " 'idea': 355,\n",
       " 'visualization': 356,\n",
       " 'basics': 357,\n",
       " 'hard': 358,\n",
       " 'may': 359,\n",
       " 'architecture': 360,\n",
       " 'set': 361,\n",
       " 'sales': 362,\n",
       " 'daily': 363,\n",
       " 'other': 364,\n",
       " 'becoming': 365,\n",
       " 'thing': 366,\n",
       " 'week': 367,\n",
       " 'investment': 368,\n",
       " 'secret': 369,\n",
       " 'quick': 370,\n",
       " 'fail': 371,\n",
       " 'happy': 372,\n",
       " 'plan': 373,\n",
       " 'journey': 374,\n",
       " 'done': 375,\n",
       " 'personal': 376,\n",
       " 'words': 377,\n",
       " 'functions': 378,\n",
       " 'problems': 379,\n",
       " 'magic': 380,\n",
       " 'small': 381,\n",
       " 'smart': 382,\n",
       " '8': 383,\n",
       " 'js': 384,\n",
       " 'which': 385,\n",
       " 'care': 386,\n",
       " 'creativity': 387,\n",
       " 'benefits': 388,\n",
       " 'live': 389,\n",
       " 'going': 390,\n",
       " 'feel': 391,\n",
       " 'mad': 392,\n",
       " 'impact': 393,\n",
       " 'email': 394,\n",
       " 'common': 395,\n",
       " 'understand': 396,\n",
       " 'actually': 397,\n",
       " 'let': 398,\n",
       " 'engineering': 399,\n",
       " 'algorithm': 400,\n",
       " 'age': 401,\n",
       " 'stock': 402,\n",
       " 'processing': 403,\n",
       " 'apple': 404,\n",
       " 'segmentation': 405,\n",
       " 'twitter': 406,\n",
       " 'culture': 407,\n",
       " 'security': 408,\n",
       " 'framework': 409,\n",
       " 'give': 410,\n",
       " 'principles': 411,\n",
       " 'multi': 412,\n",
       " 'decision': 413,\n",
       " 'full': 414,\n",
       " 'doing': 415,\n",
       " 'bitcoin': 416,\n",
       " 'taking': 417,\n",
       " 'tutorial': 418,\n",
       " 'components': 419,\n",
       " 'only': 420,\n",
       " 'scratch': 421,\n",
       " 'programming': 422,\n",
       " 'practical': 423,\n",
       " 'explained': 424,\n",
       " 'analytics': 425,\n",
       " 'taught': 426,\n",
       " 'sql': 427,\n",
       " 'off': 428,\n",
       " 'goals': 429,\n",
       " 'jobs': 430,\n",
       " 'website': 431,\n",
       " 'fear': 432,\n",
       " 'there': 433,\n",
       " 'list': 434,\n",
       " 's': 435,\n",
       " 'test': 436,\n",
       " 'java': 437,\n",
       " 'beyond': 438,\n",
       " 'service': 439,\n",
       " 'reading': 440,\n",
       " 'ii': 441,\n",
       " 'am': 442,\n",
       " 'ultimate': 443,\n",
       " 'projects': 444,\n",
       " 'scientist': 445,\n",
       " 'deal': 446,\n",
       " 'apps': 447,\n",
       " 'performance': 448,\n",
       " 'manage': 449,\n",
       " 'markets': 450,\n",
       " 'k': 451,\n",
       " 'word': 452,\n",
       " 'training': 453,\n",
       " 'setting': 454,\n",
       " 'america': 455,\n",
       " 'features': 456,\n",
       " 'mental': 457,\n",
       " 'succeed': 458,\n",
       " 'anxiety': 459,\n",
       " 'support': 460,\n",
       " 'even': 461,\n",
       " 'short': 462,\n",
       " 'block': 463,\n",
       " 'seo': 464,\n",
       " 'attention': 465,\n",
       " 'little': 466,\n",
       " 'cars': 467,\n",
       " 'agile': 468,\n",
       " 'information': 469,\n",
       " 'powerful': 470,\n",
       " 'bias': 471,\n",
       " 'convolutional': 472,\n",
       " 'perfect': 473,\n",
       " 'object': 474,\n",
       " 'types': 475,\n",
       " 'choose': 476,\n",
       " 'phone': 477,\n",
       " 'applications': 478,\n",
       " 'long': 479,\n",
       " 'music': 480,\n",
       " 'again': 481,\n",
       " 'run': 482,\n",
       " 'selection': 483,\n",
       " 'techniques': 484,\n",
       " 'implementation': 485,\n",
       " 'leaders': 486,\n",
       " 'basic': 487,\n",
       " 'stay': 488,\n",
       " 'did': 489,\n",
       " 'boost': 490,\n",
       " 'hiring': 491,\n",
       " 'past': 492,\n",
       " 'video': 493,\n",
       " 'makes': 494,\n",
       " 'entrepreneur': 495,\n",
       " 'behind': 496,\n",
       " 'works': 497,\n",
       " 'amazon': 498,\n",
       " 'beginners': 499,\n",
       " '20': 500,\n",
       " 'complete': 501,\n",
       " 'driving': 502,\n",
       " 'fast': 503,\n",
       " 'healthcare': 504,\n",
       " 'minutes': 505,\n",
       " 'cnn': 506,\n",
       " 'introducing': 507,\n",
       " 'ever': 508,\n",
       " 'visual': 509,\n",
       " 'non': 510,\n",
       " 'four': 511,\n",
       " 'thought': 512,\n",
       " '101': 513,\n",
       " 'theory': 514,\n",
       " 'vision': 515,\n",
       " 'customers': 516,\n",
       " '000': 517,\n",
       " 'relationship': 518,\n",
       " '”': 519,\n",
       " 'ethics': 520,\n",
       " 'still': 521,\n",
       " 'investing': 522,\n",
       " 'inside': 523,\n",
       " 'would': 524,\n",
       " 'health': 525,\n",
       " 'freelancer': 526,\n",
       " 'while': 527,\n",
       " 'scraping': 528,\n",
       " 'innovation': 529,\n",
       " 'economy': 530,\n",
       " 'trends': 531,\n",
       " 'sentiment': 532,\n",
       " 'running': 533,\n",
       " 'challenge': 534,\n",
       " 'breaking': 535,\n",
       " 'feedback': 536,\n",
       " '15': 537,\n",
       " 'won’t': 538,\n",
       " 'audience': 539,\n",
       " 'analyzing': 540,\n",
       " 'leader': 541,\n",
       " 'different': 542,\n",
       " 'ios': 543,\n",
       " 'businesses': 544,\n",
       " 'random': 545,\n",
       " '9': 546,\n",
       " 'method': 547,\n",
       " 'ahead': 548,\n",
       " 'post': 549,\n",
       " 'fiction': 550,\n",
       " 'win': 551,\n",
       " 'mindset': 552,\n",
       " 'ads': 553,\n",
       " 'old': 554,\n",
       " 'face': 555,\n",
       " 'matter': 556,\n",
       " 'climate': 557,\n",
       " 'modern': 558,\n",
       " 'methods': 559,\n",
       " 'come': 560,\n",
       " 'cost': 561,\n",
       " 'bot': 562,\n",
       " 'teach': 563,\n",
       " 'optimization': 564,\n",
       " 'internet': 565,\n",
       " 'quality': 566,\n",
       " 'decisions': 567,\n",
       " 'reason': 568,\n",
       " 'death': 569,\n",
       " 'news': 570,\n",
       " 'away': 571,\n",
       " 'relationships': 572,\n",
       " 'clients': 573,\n",
       " 'storytelling': 574,\n",
       " 'trying': 575,\n",
       " 'transformation': 576,\n",
       " 'trading': 577,\n",
       " 'platform': 578,\n",
       " 'keras': 579,\n",
       " 'prices': 580,\n",
       " 'knowledge': 581,\n",
       " 'price': 582,\n",
       " 'focus': 583,\n",
       " 'solve': 584,\n",
       " 'today': 585,\n",
       " 'looking': 586,\n",
       " 'many': 587,\n",
       " 'worth': 588,\n",
       " 'cryptocurrency': 589,\n",
       " 'having': 590,\n",
       " 'freelancing': 591,\n",
       " 'voice': 592,\n",
       " 'novel': 593,\n",
       " 'probably': 594,\n",
       " 'exploration': 595,\n",
       " 'facial': 596,\n",
       " 'means': 597,\n",
       " 'dream': 598,\n",
       " 'question': 599,\n",
       " 'docker': 600,\n",
       " 'spark': 601,\n",
       " 'hit': 602,\n",
       " 'well': 603,\n",
       " 'overview': 604,\n",
       " 'any': 605,\n",
       " 'rise': 606,\n",
       " 'tool': 607,\n",
       " 'last': 608,\n",
       " 'lead': 609,\n",
       " 'history': 610,\n",
       " 'helped': 611,\n",
       " 'evolution': 612,\n",
       " 'community': 613,\n",
       " 'line': 614,\n",
       " 'something': 615,\n",
       " 'planning': 616,\n",
       " 'employees': 617,\n",
       " 'workplace': 618,\n",
       " 'point': 619,\n",
       " 'accessibility': 620,\n",
       " 'increase': 621,\n",
       " 'giving': 622,\n",
       " 'sell': 623,\n",
       " 'move': 624,\n",
       " 'copy': 625,\n",
       " 'brands': 626,\n",
       " 'happiness': 627,\n",
       " 'style': 628,\n",
       " 'blogging': 629,\n",
       " 'grow': 630,\n",
       " 'publish': 631,\n",
       " 'you’ve': 632,\n",
       " 'stocks': 633,\n",
       " 'role': 634,\n",
       " 'hate': 635,\n",
       " 'someone': 636,\n",
       " 'we’re': 637,\n",
       " 'level': 638,\n",
       " 'forget': 639,\n",
       " 'microsoft': 640,\n",
       " 'difference': 641,\n",
       " 'intuition': 642,\n",
       " 'public': 643,\n",
       " 'routine': 644,\n",
       " 'days': 645,\n",
       " 'month': 646,\n",
       " 'path': 647,\n",
       " 'metrics': 648,\n",
       " 'source': 649,\n",
       " 'users': 650,\n",
       " 'friends': 651,\n",
       " 'patterns': 652,\n",
       " 'iot': 653,\n",
       " 'follow': 654,\n",
       " 'everyone': 655,\n",
       " 'images': 656,\n",
       " 'modeling': 657,\n",
       " 'paper': 658,\n",
       " 'exploratory': 659,\n",
       " 'native': 660,\n",
       " '12': 661,\n",
       " 'natural': 662,\n",
       " 'perspective': 663,\n",
       " 'solving': 664,\n",
       " 'weekly': 665,\n",
       " 'risk': 666,\n",
       " 'teams': 667,\n",
       " 'shouldn’t': 668,\n",
       " 'autonomous': 669,\n",
       " 'augmented': 670,\n",
       " 'smarter': 671,\n",
       " 'neighbors': 672,\n",
       " 'matters': 673,\n",
       " 'instead': 674,\n",
       " 'beginner’s': 675,\n",
       " 'automate': 676,\n",
       " 'reviews': 677,\n",
       " 'black': 678,\n",
       " 'dark': 679,\n",
       " 'experiences': 680,\n",
       " 'front': 681,\n",
       " 'meaning': 682,\n",
       " 'stuck': 683,\n",
       " 'purpose': 684,\n",
       " 'person': 685,\n",
       " 'gets': 686,\n",
       " 'report': 687,\n",
       " 'investor': 688,\n",
       " 'dimension': 689,\n",
       " 'pipeline': 690,\n",
       " 'used': 691,\n",
       " 'optimizing': 692,\n",
       " 'clustering': 693,\n",
       " 'forecasting': 694,\n",
       " 'employee': 695,\n",
       " 'choosing': 696,\n",
       " 'remote': 697,\n",
       " 'ddi': 698,\n",
       " 'meetings': 699,\n",
       " 'scale': 700,\n",
       " 'fix': 701,\n",
       " 'chatbot': 702,\n",
       " 'page': 703,\n",
       " 'intro': 704,\n",
       " 'statistical': 705,\n",
       " 'saving': 706,\n",
       " '50': 707,\n",
       " 'six': 708,\n",
       " 'essential': 709,\n",
       " 'try': 710,\n",
       " 'course': 711,\n",
       " 'interactive': 712,\n",
       " 'products': 713,\n",
       " 'pitch': 714,\n",
       " 'angular': 715,\n",
       " 'biggest': 716,\n",
       " '30': 717,\n",
       " 'income': 718,\n",
       " 'car': 719,\n",
       " 'negative': 720,\n",
       " 'amp': 721,\n",
       " 'exploring': 722,\n",
       " 'overcome': 723,\n",
       " 'humans': 724,\n",
       " 'math': 725,\n",
       " 'against': 726,\n",
       " 'computing': 727,\n",
       " 'author': 728,\n",
       " 'blog': 729,\n",
       " 'generative': 730,\n",
       " 'debt': 731,\n",
       " 'yet': 732,\n",
       " 'enterprise': 733,\n",
       " 'lost': 734,\n",
       " 'influence': 735,\n",
       " 'nearest': 736,\n",
       " 'youtube': 737,\n",
       " 'netflix': 738,\n",
       " 'automation': 739,\n",
       " 'graph': 740,\n",
       " 'matrix': 741,\n",
       " 'recommender': 742,\n",
       " 'single': 743,\n",
       " 'playing': 744,\n",
       " 'silicon': 745,\n",
       " 'usability': 746,\n",
       " 'show': 747,\n",
       " 'rules': 748,\n",
       " 'skill': 749,\n",
       " 'friend': 750,\n",
       " 'master': 751,\n",
       " 'inner': 752,\n",
       " 'mistake': 753,\n",
       " 'comparing': 754,\n",
       " 'another': 755,\n",
       " 'reduce': 756,\n",
       " 'because': 757,\n",
       " 'lstm': 758,\n",
       " 'autoencoders': 759,\n",
       " 'dataset': 760,\n",
       " 'u': 761,\n",
       " 'map': 762,\n",
       " 'values': 763,\n",
       " 'challenges': 764,\n",
       " 'results': 765,\n",
       " 'rich': 766,\n",
       " 'goal': 767,\n",
       " 'play': 768,\n",
       " 'developer': 769,\n",
       " 'develop': 770,\n",
       " 'e': 771,\n",
       " 'potential': 772,\n",
       " 'stand': 773,\n",
       " 'pro': 774,\n",
       " 'been': 775,\n",
       " 'changed': 776,\n",
       " 'mobility': 777,\n",
       " 'you’ll': 778,\n",
       " 'technical': 779,\n",
       " 'turning': 780,\n",
       " 'speed': 781,\n",
       " 'anything': 782,\n",
       " 'budget': 783,\n",
       " 'statistics': 784,\n",
       " 'publishing': 785,\n",
       " 'managing': 786,\n",
       " 'worst': 787,\n",
       " 'letter': 788,\n",
       " 'games': 789,\n",
       " 'robots': 790,\n",
       " 'empathy': 791,\n",
       " 'hack': 792,\n",
       " 'term': 793,\n",
       " 'survival': 794,\n",
       " 'prediction': 795,\n",
       " 'welcome': 796,\n",
       " 'vue': 797,\n",
       " 'maps': 798,\n",
       " 'road': 799,\n",
       " 'rest': 800,\n",
       " 'mapping': 801,\n",
       " 'far': 802,\n",
       " 'boss': 803,\n",
       " 'quantum': 804,\n",
       " 'political': 805,\n",
       " 'beautiful': 806,\n",
       " 'city': 807,\n",
       " 'hidden': 808,\n",
       " '11': 809,\n",
       " 'habit': 810,\n",
       " 'tricks': 811,\n",
       " 'food': 812,\n",
       " 'trick': 813,\n",
       " 'action': 814,\n",
       " 'generate': 815,\n",
       " 'coding': 816,\n",
       " 'around': 817,\n",
       " 'developing': 818,\n",
       " 'myself': 819,\n",
       " 'became': 820,\n",
       " 'crypto': 821,\n",
       " 'influencers': 822,\n",
       " 'notes': 823,\n",
       " 'times': 824,\n",
       " 'kaggle': 825,\n",
       " 'faster': 826,\n",
       " 'efficient': 827,\n",
       " 'fake': 828,\n",
       " 'practice': 829,\n",
       " 'fun': 830,\n",
       " 'rate': 831,\n",
       " 'turn': 832,\n",
       " 'stress': 833,\n",
       " 'call': 834,\n",
       " 'sense': 835,\n",
       " 'losing': 836,\n",
       " 'selling': 837,\n",
       " 'beat': 838,\n",
       " 'meeting': 839,\n",
       " 'identity': 840,\n",
       " 'following': 841,\n",
       " 'college': 842,\n",
       " 'latest': 843,\n",
       " 'expert': 844,\n",
       " 'reduction': 845,\n",
       " 'growing': 846,\n",
       " 'predict': 847,\n",
       " 'dynamic': 848,\n",
       " 'portfolio': 849,\n",
       " 'traffic': 850,\n",
       " 'validation': 851,\n",
       " 'store': 852,\n",
       " 'wish': 853,\n",
       " 'waste': 854,\n",
       " 'prepare': 855,\n",
       " 'chain': 856,\n",
       " 'net': 857,\n",
       " 'lives': 858,\n",
       " 'thanks': 859,\n",
       " 'commerce': 860,\n",
       " 'eyes': 861,\n",
       " 'winning': 862,\n",
       " 'asking': 863,\n",
       " 'libraries': 864,\n",
       " 'transfer': 865,\n",
       " 'learnt': 866,\n",
       " 'github': 867,\n",
       " 'illusion': 868,\n",
       " 'example': 869,\n",
       " 'deploy': 870,\n",
       " 'multiple': 871,\n",
       " 'government': 872,\n",
       " 'pipelines': 873,\n",
       " 'selenium': 874,\n",
       " 'ready': 875,\n",
       " 'traditional': 876,\n",
       " 'memory': 877,\n",
       " 'practices': 878,\n",
       " 'edge': 879,\n",
       " 'early': 880,\n",
       " 'energy': 881,\n",
       " 'fall': 882,\n",
       " 'rejection': 883,\n",
       " 'toxic': 884,\n",
       " 'track': 885,\n",
       " 'pyspark': 886,\n",
       " 'environment': 887,\n",
       " 'article': 888,\n",
       " 'sleep': 889,\n",
       " 'examples': 890,\n",
       " 'hire': 891,\n",
       " 'failure': 892,\n",
       " 'see': 893,\n",
       " 'manager': 894,\n",
       " 'same': 895,\n",
       " 'engineer': 896,\n",
       " 'b': 897,\n",
       " 'virtual': 898,\n",
       " 'primer': 899,\n",
       " 'measure': 900,\n",
       " 'fit': 901,\n",
       " 'choice': 902,\n",
       " 'there’s': 903,\n",
       " 'aren’t': 904,\n",
       " 'professional': 905,\n",
       " 'bert': 906,\n",
       " 'easier': 907,\n",
       " 'tale': 908,\n",
       " 'share': 909,\n",
       " 'low': 910,\n",
       " 'optimize': 911,\n",
       " 'adversarial': 912,\n",
       " '100': 913,\n",
       " 'yes': 914,\n",
       " 'creation': 915,\n",
       " 'credit': 916,\n",
       " 'improving': 917,\n",
       " 'months': 918,\n",
       " 'branding': 919,\n",
       " 'raspberry': 920,\n",
       " 'his': 921,\n",
       " 'bigquery': 922,\n",
       " 'workflow': 923,\n",
       " 'pi': 924,\n",
       " 'freelancers': 925,\n",
       " 'advanced': 926,\n",
       " 'major': 927,\n",
       " 'per': 928,\n",
       " 'git': 929,\n",
       " 'churn': 930,\n",
       " 'cross': 931,\n",
       " 'apache': 932,\n",
       " 'bring': 933,\n",
       " 'instance': 934,\n",
       " 'didn’t': 935,\n",
       " 'corporate': 936,\n",
       " 'local': 937,\n",
       " 'were': 938,\n",
       " '40': 939,\n",
       " 'view': 940,\n",
       " 'some': 941,\n",
       " 'color': 942,\n",
       " 'developers': 943,\n",
       " 'implementing': 944,\n",
       " 'engagement': 945,\n",
       " 'needed': 946,\n",
       " 't': 947,\n",
       " 'rails': 948,\n",
       " 'head': 949,\n",
       " 'keys': 950,\n",
       " 'law': 951,\n",
       " 'policy': 952,\n",
       " 'artist': 953,\n",
       " 'shape': 954,\n",
       " 'difficult': 955,\n",
       " 'fintech': 956,\n",
       " 'demand': 957,\n",
       " 'alternative': 958,\n",
       " 'bought': 959,\n",
       " 'production': 960,\n",
       " 'robot': 961,\n",
       " 'recommendation': 962,\n",
       " 'visualizing': 963,\n",
       " 'detecting': 964,\n",
       " 'sampling': 965,\n",
       " 'missing': 966,\n",
       " 'function': 967,\n",
       " 'confidence': 968,\n",
       " 'knew': 969,\n",
       " 'embrace': 970,\n",
       " 'quit': 971,\n",
       " 'solution': 972,\n",
       " 'truth': 973,\n",
       " 'revolution': 974,\n",
       " 'china': 975,\n",
       " 'procrastination': 976,\n",
       " 'dollar': 977,\n",
       " 'critical': 978,\n",
       " 'error': 979,\n",
       " 'entrepreneurs': 980,\n",
       " '5g': 981,\n",
       " 'syndrome': 982,\n",
       " 'topic': 983,\n",
       " 'almost': 984,\n",
       " 'serverless': 985,\n",
       " 'numpy': 986,\n",
       " 'analyze': 987,\n",
       " 'influencer': 988,\n",
       " 'towards': 989,\n",
       " 'global': 990,\n",
       " 'secrets': 991,\n",
       " 'order': 992,\n",
       " 'flow': 993,\n",
       " 'communication': 994,\n",
       " 'crisis': 995,\n",
       " 'already': 996,\n",
       " 'super': 997,\n",
       " '21st': 998,\n",
       " 'century': 999,\n",
       " 'services': 1000,\n",
       " ...}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.word_index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61daad01",
   "metadata": {},
   "source": [
    "## Convert Text To Sequences\n",
    "\n",
    "\n",
    "Convert Titles to Sequences: Use a tokenizer to turn each title into a string of tokens or manually separate each slip into its constituent words. Assign each word in the lexicon a distinct number index.\n",
    "\n",
    "Generate n-grams: From the sequences, make n-grams. A continuous run of n-title tokens is called an n-gram.\n",
    "\n",
    "Count the Frequency: Determine the frequency at which each n-gram appears in the dataset.\n",
    "\n",
    "Build the n-gram Model: Create the n-gram model using the n-gram frequencies. The model keeps track of each token probability given the previous n-1 tokens. This can be displayed as a lookup table or a dictionary.\n",
    "\n",
    "Predict the Next Word: The expected next token in an n-1-token sequence may be identified using the n-gram model. To do this, it is necessary to find the probability in the algorithm and select a token with the greatest likelihood.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "53c3fce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_seq= []\n",
    "# for line in df['title']:\n",
    "input_seq.append(tokenizer.texts_to_sequences(df['clean_data']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "75dd8d47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(input_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1eb26b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_seq[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "668bb94e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6508"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(input_seq[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1d60b0f",
   "metadata": {},
   "source": [
    "## Make all the Sequences as same length By Padding\n",
    "\n",
    "\n",
    "\n",
    "Find the longest title in your dataset by comparing all the other titles.\n",
    "\n",
    "Repeat this process for each title, comparing each one’s length to the overall limit.\n",
    "\n",
    "When a title is too short, it should be extended using a specific padding token or character.\n",
    "\n",
    "For each title in your dataset, carry out the padding procedure again.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cd882c28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_seq_len: 40\n",
      "Total input sequences:  6508\n"
     ]
    }
   ],
   "source": [
    "# pad sequences\n",
    "max_seq_len= max([len(x) for x in input_seq[0]])\n",
    "print(\"max_seq_len:\", max_seq_len)\n",
    "\n",
    "input_seq= pad_sequences(input_seq[0], maxlen= max_seq_len, padding= 'pre')\n",
    "print(\"Total input sequences: \", len(input_seq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "313bde96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6508, 40)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_seq.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90af392f",
   "metadata": {},
   "source": [
    "### Prepare Features and Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "261735e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " shape of xs: ((6508, 39), 6508) \n",
      "\n",
      " shape of xs[1]: 39 \n",
      "\n",
      " current encoded words-> xs[1]: [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0 1858   22  740   80  102   13  344  344] \n",
      "\n",
      "\n",
      " shape of labels: ((6508,), 6508) \n",
      "\n",
      "next time-step-> labels[1]: 1859 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# create a timestep to predict a next word\n",
    "xs, labels= input_seq[:,:-1], input_seq[:,-1]\n",
    "xs_= np.array(xs)\n",
    "labels_= np.array(labels)\n",
    "\n",
    "print(f\"\\n shape of xs: {xs_.shape,len(xs)} \" )\n",
    "print(f\"\\n shape of xs[1]: {len(xs[1])} \\n\\n current encoded words-> xs[1]:\",xs[1], \"\\n\")\n",
    "print(f\"\\n shape of labels: {labels_.shape, len(labels)} \\n\\nnext time-step-> labels[1]:\",labels[1], '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e766aa36",
   "metadata": {},
   "source": [
    "### Bi-Directional LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "52de4efb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "WARNING:tensorflow:From C:\\Users\\varsha\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\varsha\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "204/204 [==============================] - 12s 48ms/step - loss: 8.0679 - accuracy: 0.0616\n",
      "Epoch 2/5\n",
      "204/204 [==============================] - 9s 47ms/step - loss: 6.6309 - accuracy: 0.1073\n",
      "Epoch 3/5\n",
      "204/204 [==============================] - 10s 47ms/step - loss: 5.4630 - accuracy: 0.1501\n",
      "Epoch 4/5\n",
      "204/204 [==============================] - 10s 48ms/step - loss: 4.0198 - accuracy: 0.2449\n",
      "Epoch 5/5\n",
      "204/204 [==============================] - 10s 47ms/step - loss: 2.4392 - accuracy: 0.4677\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(total_words, 100, input_length= max_seq_len-1))\n",
    "model.add(Bidirectional(LSTM(150)))\n",
    "model.add(Dense(total_words, activation= 'softmax'))\n",
    "adam= Adam(learning_rate= 0.01)\n",
    "\n",
    "model.compile(loss= 'sparse_categorical_crossentropy', optimizer= adam, metrics= ['accuracy'])\n",
    "history= model.fit(xs, labels, epochs= 5, verbose= 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aebee2ce",
   "metadata": {},
   "source": [
    "### Plot the Model Accuracy and Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed5f1af4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def graph_plot(history, string):\n",
    "    plt.plot(history.history[string])\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(string)\n",
    "    plt.show()\n",
    "    \n",
    "graph_plot(history, 'accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24632e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_plot(history, 'loss')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5583a7f1",
   "metadata": {},
   "source": [
    "### Predicting Next Word\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f79fe670",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_text='implementation of'\n",
    "next_words= 2\n",
    "\n",
    "for _ in range(next_words):\n",
    "    token_list= tokenizer.texts_to_sequences([seed_text])[0]\n",
    "    print(token_list)\n",
    "    token_list= pad_sequences([token_list], maxlen= max_seq_len-1, padding= 'pre')\n",
    "    print(len(token_list))\n",
    "    predicted= model.predict(token_list, verbose= 0)\n",
    "    pred_class= np.argmax(predicted)\n",
    "    print(pred_class)\n",
    "    output_word= ''\n",
    "    \n",
    "    for word, index in tokenizer.word_index.items():\n",
    "#         print(word)\n",
    "        if index == pred_class:\n",
    "            output_word= word\n",
    "            break\n",
    "            \n",
    "    seed_text+= \" \"+output_word\n",
    "    \n",
    "print(seed_text)\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1e06a92",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9c2812b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc99bec5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2406990",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "494bca23",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2822aa3c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13fa6b6a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba703c8d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64b15946",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60ec8e5b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc2a7c62",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4e9b779",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fca3d67",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
