{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "f460b5ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "\n",
    "# library to visualize text data\n",
    "from wordcloud import WordCloud\n",
    "#a collection of words that donâ€™t provide any meaning to a sentence\n",
    "from nltk.corpus import stopwords\n",
    "#used to convert different forms of words into a single item but still keeping the context intact.\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "from keras.layers import Conv1D, Input, Layer, Dense, Activation, Dropout\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "#used to convert different forms of words into a single item but still keeping the context intact.\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "nlp= spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "32c35985",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# load Dataset\n",
    "\n",
    "df= pd.read_csv(r\"D:\\Datasets\\Twitter Tweets\\training.1600000.processed.noemoticon.csv\", encoding='latin-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "0e8a9139",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1467810369</th>\n",
       "      <th>Mon Apr 06 22:19:45 PDT 2009</th>\n",
       "      <th>NO_QUERY</th>\n",
       "      <th>_TheSpecialOne_</th>\n",
       "      <th>@switchfoot http://twitpic.com/2y1zl - Awww, that's a bummer.  You shoulda got David Carr of Third Day to do it. ;D</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810672</td>\n",
       "      <td>Mon Apr 06 22:19:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>scotthamilton</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810917</td>\n",
       "      <td>Mon Apr 06 22:19:53 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>mattycus</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811184</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>ElleCTF</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811193</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Karoli</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811372</td>\n",
       "      <td>Mon Apr 06 22:20:00 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>joy_wolf</td>\n",
       "      <td>@Kwesidei not the whole crew</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0  ...  @switchfoot http://twitpic.com/2y1zl - Awww, that's a bummer.  You shoulda got David Carr of Third Day to do it. ;D\n",
       "0  0  ...  is upset that he can't update his Facebook by ...                                                                  \n",
       "1  0  ...  @Kenichan I dived many times for the ball. Man...                                                                  \n",
       "2  0  ...    my whole body feels itchy and like its on fire                                                                   \n",
       "3  0  ...  @nationwideclass no, it's not behaving at all....                                                                  \n",
       "4  0  ...                      @Kwesidei not the whole crew                                                                   \n",
       "\n",
       "[5 rows x 6 columns]"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "08198a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data in X and Y\n",
    "\n",
    "X= df[df.columns[5]]\n",
    "y= df[df.columns[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "bae84b65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19994    4\n",
       "19995    4\n",
       "19996    4\n",
       "19997    4\n",
       "19998    4\n",
       "Name: 0, dtype: int64"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "3e5b3370",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    is upset that he can't update his Facebook by ...\n",
       "1    @Kenichan I dived many times for the ball. Man...\n",
       "2      my whole body feels itchy and like its on fire \n",
       "3    @nationwideclass no, it's not behaving at all....\n",
       "4                        @Kwesidei not the whole crew \n",
       "Name: @switchfoot http://twitpic.com/2y1zl - Awww, that's a bummer.  You shoulda got David Carr of Third Day to do it. ;D, dtype: object"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "c857dfa6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19999,)"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "b3121309",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split trhe dataset into train and test data\n",
    "trainset1x, trainset2x, trainset1y, trainset2y = train_test_split(X, y, test_size= 0.02, random_state= 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "f4ffa845",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(((19599,), (19599,)), ((400,), (400,)))"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(trainset1x.shape,  trainset1y.shape),(trainset2x.shape, trainset2y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "8ddeb8be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we will be using the smaller train set for preprocessing and modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "17a35a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get dummies for y of train set 2 -- this is done to use it in rnn netweork\n",
    "trainset2y=pd.get_dummies(trainset2y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "6b7a6511",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Preprocessing 1st way\n",
    "def data_preprocess_1st_way(doc):\n",
    "    \n",
    "    corpus= []\n",
    "    doc= nlp(doc)\n",
    "    \n",
    "    # regex pattern\n",
    "    pattern= r'[^a-zA-Z0-9\\s]'  # keep letters, digit and whitespace\n",
    "    \n",
    "    for word in doc:\n",
    "        \n",
    "        # remove special characters using regex sub()\n",
    "        clean_word= re.sub(pattern, '', word)\n",
    "        \n",
    "        # convert to lower case\n",
    "        clean_word= word.lower()\n",
    "        \n",
    "        # two ways of doing tokenization\n",
    "        clean_word= word.split()\n",
    "        \n",
    "        # perform lemmetazation and remove stop words\n",
    "        lemma= WordNetLemmatizer()\n",
    "        clean_word= [ lemma.lemmatize(i) for i in clean_word if i not in set(stopwords.words('english'))]\n",
    "        \n",
    "        # join the words to form corpus\n",
    "        corpus.append(' '.join(str(x) for i in clean_word))\n",
    "       \n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "7e5f287e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load glove model to convert word into vector\n",
    "\n",
    "def loadGloveModel(gloveFile):\n",
    "    print('Loading glove model')\n",
    "    f= open(gloveFile, 'r', encoding='utf-8')\n",
    "    print(f)\n",
    "    model= {}\n",
    "    \n",
    "    for line in f:\n",
    "        splitLine= line.split()\n",
    "        word= splitLine[0]\n",
    "        embedding= [float(val) for val in splitLine[1:]]\n",
    "        model[word]= embedding\n",
    "        \n",
    "    print(\"Done.\",len(model),\" words loaded!\")\n",
    "        \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "2bf304bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading glove model\n",
      "<_io.TextIOWrapper name='D:\\\\Datasets\\\\glove.6B\\\\glove.6B.300d.txt' mode='r' encoding='utf-8'>\n",
      "Done. 400000  words loaded!\n"
     ]
    }
   ],
   "source": [
    "# save the glove model\n",
    "model= loadGloveModel(r\"D:\\Datasets\\glove.6B\\glove.6B.300d.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "f36166aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(model['the'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "51d2753a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "400000"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(model.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "eca8dfa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorize the sentence\n",
    "def sent_Vectorize(sentence, model):\n",
    "    sent_vect= np.zeros(300)\n",
    "    numw= 0\n",
    "    for word in sentence.split():\n",
    "        try:\n",
    "            sent_vect= np.add(sent_vect, model[str(word)])\n",
    "            numw+=1\n",
    "            \n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    return sent_vect\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "8ed298c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Preprocessing 2nd way\n",
    "def data_preprocess_2nd_way(document):\n",
    "    \n",
    "    corpus= []\n",
    "    \n",
    "    for word in document:\n",
    "        \n",
    "        # convert to lower case\n",
    "        clean_word= word.lower()\n",
    "        \n",
    "        # lemmatize the word\n",
    "        lemma= WordNetLemmatizer()\n",
    "        clean_word= lemma.lemmatize(clean_word)\n",
    "        clean_word= str(clean_word)\n",
    "        corpus.append(sent_Vectorize(clean_word, model))\n",
    "    \n",
    "    # getting input and output in proper sequence\n",
    "    cleanVector= np.array(corpus)\n",
    "    cleanVector= cleanVector.reshape(len(cleanVector), 300, 1)\n",
    "        \n",
    "    # tokenize the word\n",
    "    tokenizer= Tokenizer(num_words= 16000)\n",
    "    tokenizer.fit_on_texts(document)\n",
    "    sequences= tokenizer.texts_to_sequences(document)\n",
    "\n",
    "    word_index= tokenizer.word_index\n",
    "    print('Found %s unique tokens.' % len(word_index))\n",
    "\n",
    "    padded_data= pad_sequences(sequences, maxlen= 15, padding= 'post')\n",
    "    print(padded_data.shape)\n",
    "    \n",
    "    # reshape the data and prepare to train\n",
    "    data= padded_data.reshape(len(cleanVector), 15, 1)\n",
    "    \n",
    "    \n",
    "    return data, tokenizer, word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "2200a3dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1873 unique tokens.\n",
      "(400, 15)\n"
     ]
    }
   ],
   "source": [
    "# We will be using 2nd way for preprocessing\n",
    "data, tokenizer, word_index= data_preprocess_2nd_way(trainset2x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "0cc94760",
   "metadata": {},
   "outputs": [],
   "source": [
    "# spli the data into train and test\n",
    "trainx, validx, trainy, validy = train_test_split(data, trainset2y, test_size=0.3,random_state=42 )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "e1f3d952",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1874"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate the number of words\n",
    "nb_words= len(tokenizer.word_index)+1\n",
    "nb_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "2566913a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Null word embeddings: 346\n"
     ]
    }
   ],
   "source": [
    "# obtain the embedding matrix for embedding layer\n",
    "embedding_matrix= np.zeros((nb_words, 300))\n",
    "\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector= model.get(word)\n",
    "    \n",
    "#     print(len(embedding_vector))\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i]= embedding_vector\n",
    "    \n",
    "print('Null word embeddings: %d' % np.sum(np.sum(embedding_matrix, axis=1) == 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "22cf9535",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1874, 300)"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "8b738f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainy=np.array(trainy)\n",
    "validy=np.array(validy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "35631a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# building a simple RNN model\n",
    "\n",
    "def modelBuild():\n",
    "    \n",
    "    model= keras.Sequential()\n",
    "    model.add(keras.layers.InputLayer(input_shape= (15,1)))\n",
    "    keras.layers.Embedding(nb_words, 15, input_length= 15, weights= [embedding_matrix], trainable= False)\n",
    "    \n",
    "    model.add(keras.layers.SimpleRNN(units= 100, activation= 'relu', use_bias=True))\n",
    "    model.add(Dense(units=1000, input_dim=2000, activation= 'relu'))\n",
    "    model.add(keras.layers.Dense(units=500, input_dim=1000, activation='relu'))\n",
    "    model.add(keras.layers.Dense(units=2, input_dim=500,activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "ec6d2b93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "3/3 [==============================] - 1s 117ms/step - loss: 61.3678 - accuracy: 0.4786 - val_loss: 35.4877 - val_accuracy: 0.4917\n",
      "Epoch 2/50\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 15.1439 - accuracy: 0.4857 - val_loss: 29.4011 - val_accuracy: 0.5083\n",
      "Epoch 3/50\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 25.7829 - accuracy: 0.5321 - val_loss: 20.1906 - val_accuracy: 0.5083\n",
      "Epoch 4/50\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 11.9644 - accuracy: 0.5321 - val_loss: 5.4798 - val_accuracy: 0.5333\n",
      "Epoch 5/50\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 6.6475 - accuracy: 0.5179 - val_loss: 11.0740 - val_accuracy: 0.4917\n",
      "Epoch 6/50\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 8.5636 - accuracy: 0.4679 - val_loss: 4.5939 - val_accuracy: 0.4917\n",
      "Epoch 7/50\n",
      "3/3 [==============================] - 0s 23ms/step - loss: 2.6245 - accuracy: 0.4786 - val_loss: 3.4321 - val_accuracy: 0.5083\n",
      "Epoch 8/50\n",
      "3/3 [==============================] - 0s 22ms/step - loss: 3.3826 - accuracy: 0.5857 - val_loss: 3.5510 - val_accuracy: 0.5250\n",
      "Epoch 9/50\n",
      "3/3 [==============================] - 0s 22ms/step - loss: 2.6610 - accuracy: 0.6071 - val_loss: 0.8370 - val_accuracy: 0.4917\n",
      "Epoch 10/50\n",
      "3/3 [==============================] - 0s 24ms/step - loss: 1.4446 - accuracy: 0.5643 - val_loss: 2.5213 - val_accuracy: 0.5083\n",
      "Epoch 11/50\n",
      "3/3 [==============================] - 0s 22ms/step - loss: 1.9326 - accuracy: 0.5571 - val_loss: 0.9255 - val_accuracy: 0.5500\n",
      "Epoch 12/50\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 1.1133 - accuracy: 0.6107 - val_loss: 1.4991 - val_accuracy: 0.5083\n",
      "Epoch 13/50\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 1.2127 - accuracy: 0.6107 - val_loss: 1.1562 - val_accuracy: 0.5500\n",
      "Epoch 14/50\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.1869 - accuracy: 0.6429 - val_loss: 1.5133 - val_accuracy: 0.5583\n",
      "Epoch 15/50\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.9813 - accuracy: 0.6286 - val_loss: 1.1940 - val_accuracy: 0.5167\n",
      "Epoch 16/50\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 1.0252 - accuracy: 0.6357 - val_loss: 1.1003 - val_accuracy: 0.4917\n",
      "Epoch 17/50\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.7885 - accuracy: 0.6250 - val_loss: 1.0773 - val_accuracy: 0.5583\n",
      "Epoch 18/50\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.7028 - accuracy: 0.6750 - val_loss: 0.9639 - val_accuracy: 0.4917\n",
      "Epoch 19/50\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 0.7345 - accuracy: 0.6429 - val_loss: 1.0446 - val_accuracy: 0.5333\n",
      "Epoch 20/50\n",
      "3/3 [==============================] - 0s 23ms/step - loss: 0.6916 - accuracy: 0.6714 - val_loss: 0.9797 - val_accuracy: 0.5083\n",
      "Epoch 21/50\n",
      "3/3 [==============================] - 0s 22ms/step - loss: 0.6695 - accuracy: 0.6857 - val_loss: 0.9678 - val_accuracy: 0.4917\n",
      "Epoch 22/50\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.6183 - accuracy: 0.6464 - val_loss: 1.1014 - val_accuracy: 0.5333\n",
      "Epoch 23/50\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.5882 - accuracy: 0.7000 - val_loss: 1.0343 - val_accuracy: 0.5250\n",
      "Epoch 24/50\n",
      "3/3 [==============================] - 0s 22ms/step - loss: 0.5638 - accuracy: 0.6786 - val_loss: 1.1492 - val_accuracy: 0.5583\n",
      "Epoch 25/50\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 0.5419 - accuracy: 0.7143 - val_loss: 1.0735 - val_accuracy: 0.4917\n",
      "Epoch 26/50\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 0.5681 - accuracy: 0.7107 - val_loss: 1.0867 - val_accuracy: 0.5250\n",
      "Epoch 27/50\n",
      "3/3 [==============================] - 0s 22ms/step - loss: 0.5499 - accuracy: 0.7214 - val_loss: 1.0162 - val_accuracy: 0.5250\n",
      "Epoch 28/50\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.5285 - accuracy: 0.7214 - val_loss: 1.0222 - val_accuracy: 0.5167\n",
      "Epoch 29/50\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.4875 - accuracy: 0.7393 - val_loss: 1.0359 - val_accuracy: 0.5083\n",
      "Epoch 30/50\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 0.5027 - accuracy: 0.7357 - val_loss: 1.2345 - val_accuracy: 0.5000\n",
      "Epoch 31/50\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 0.5854 - accuracy: 0.7571 - val_loss: 1.1782 - val_accuracy: 0.4917\n",
      "Epoch 32/50\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.5338 - accuracy: 0.7607 - val_loss: 1.3497 - val_accuracy: 0.4917\n",
      "Epoch 33/50\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 0.5589 - accuracy: 0.7714 - val_loss: 1.5084 - val_accuracy: 0.5000\n",
      "Epoch 34/50\n",
      "3/3 [==============================] - 0s 22ms/step - loss: 0.4881 - accuracy: 0.8214 - val_loss: 1.3046 - val_accuracy: 0.4417\n",
      "Epoch 35/50\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 0.4689 - accuracy: 0.8214 - val_loss: 1.6605 - val_accuracy: 0.4750\n",
      "Epoch 36/50\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 0.5925 - accuracy: 0.8000 - val_loss: 1.5925 - val_accuracy: 0.5000\n",
      "Epoch 37/50\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.4470 - accuracy: 0.8607 - val_loss: 1.4147 - val_accuracy: 0.4667\n",
      "Epoch 38/50\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 0.4664 - accuracy: 0.8036 - val_loss: 1.6280 - val_accuracy: 0.4500\n",
      "Epoch 39/50\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.4163 - accuracy: 0.8286 - val_loss: 1.4069 - val_accuracy: 0.4333\n",
      "Epoch 40/50\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.4168 - accuracy: 0.8500 - val_loss: 1.6594 - val_accuracy: 0.4667\n",
      "Epoch 41/50\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.3835 - accuracy: 0.8464 - val_loss: 1.5069 - val_accuracy: 0.4667\n",
      "Epoch 42/50\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 0.3422 - accuracy: 0.8500 - val_loss: 1.4820 - val_accuracy: 0.4750\n",
      "Epoch 43/50\n",
      "3/3 [==============================] - 0s 22ms/step - loss: 0.3910 - accuracy: 0.8500 - val_loss: 1.6823 - val_accuracy: 0.4833\n",
      "Epoch 44/50\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.3450 - accuracy: 0.8464 - val_loss: 1.8716 - val_accuracy: 0.4250\n",
      "Epoch 45/50\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.3405 - accuracy: 0.8571 - val_loss: 1.8884 - val_accuracy: 0.4833\n",
      "Epoch 46/50\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.3154 - accuracy: 0.8643 - val_loss: 1.9675 - val_accuracy: 0.4750\n",
      "Epoch 47/50\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 0.2769 - accuracy: 0.8821 - val_loss: 2.0094 - val_accuracy: 0.4583\n",
      "Epoch 48/50\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.2555 - accuracy: 0.8679 - val_loss: 1.8881 - val_accuracy: 0.4583\n",
      "Epoch 49/50\n",
      "3/3 [==============================] - 0s 23ms/step - loss: 0.2624 - accuracy: 0.8893 - val_loss: 1.8966 - val_accuracy: 0.4833\n",
      "Epoch 50/50\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.2467 - accuracy: 0.8893 - val_loss: 2.0742 - val_accuracy: 0.4750\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x1ea18e99550>"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#compiling the model\n",
    "finalmodel = modelBuild()\n",
    "finalmodel.fit(trainx, trainy, epochs=50, batch_size=120,validation_data=(validx,validy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d5677b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b2993e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "075db4ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a09c9c23",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "786d98bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bde56b94",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8c78c50",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d8998f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11c4c600",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
